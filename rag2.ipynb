{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CheatBot\n",
    "##### The primary goal of this bot is to generate answers based on a piece of data given.\n",
    "\n",
    "## TechStack\n",
    "Gemini LLm <br>\n",
    "Langchain <br>\n",
    "GeminiEmbeddings <br>\n",
    "ChromaDB as a Vector Database<br>\n",
    "Various other python libraries to enhancing the bot<br>\n",
    "<br>\n",
    "#### - Developed by Mihiresh Joshi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install all these libraries using pip command\n",
    "# Here I made your work easy (use 'pip3' if on pip3 version): \n",
    "# pip install PyPDF2 langchain python-dotenv pandas chromadb google-api-python-client tqdm\n",
    "\n",
    "import PyPDF2\n",
    "import langchain\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown\n",
    "from chromadb.api.types import Embeddings\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from google.generativeai import GenerationConfig, GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"api key\"  # Change it with your API key\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Management & Visualization (DMV) Module 1Structured Query Language (SQL)Index\n",
      "2Lecture  1Overview of SQL4Lecture  1Data Types, Data Definition Commands. 16Lecture  2Data Manipulation commands32Lecture  2Integrity constraints ; Referential integrity , check constraints 38Lecture 1 Overview of SQLOverview of SQL- What is SQL?§Standard command set used to communicate with relational DBMS.§ All tasks related to Relational Database Management like creating table, querying the database for information , modifying data in the database.Advantages§High level  language that provides a greater degree of abstraction than procedural languages.§Allow the end-users to deal with number of database management systems where it is available.§Simple and easy to learn can handle complex situation.§All SQL operations are performed at set level.§For digital systems, the variable takes on discrete values.4Data Models\n",
      "5Model ofsystemin client’smindEntity model ofclient’s modelTable modelof entity modelTables on diskOracleserverDefinition of a Relational Database\n",
      "Relational Model and relational Algebra6A relational database is a collection of relations or two-dimensional tables.\n",
      "Oracle\n",
      "serverTable Name: EMPLOYEESTable Name: DEPARTMENTS……\n",
      "7Relational Database Management System\n",
      "User tablesData dictionaryServer8Communicating with a RDBMS Using SQL\n",
      "SELECT department_name FROM   departments;SQL statementis entered.OracleserverStatement is sent to Oracle Server.\n",
      "Advantages of DBMS§No redundant data §Data Consistency§Secure§Privacy – Limited access§Easy access to data§Easy recovery\n",
      "9Relational Model \n",
      "10NameAddressPhone_noDOBSemester Smith...IIIJones...IIICurry...IIIJack...IIIStudentRelation\n",
      "Record/tupleColumn/Attribute\n",
      "Domain Example 2-Employee Relation\n",
      "11IDNameAgeSalary1Adam34130002Alex28150003Stuart20180004Ross4219020What is a Record/Tuple, Column/Attribute, Domain?\n",
      "121Adam3413000§ A single entry in a table is called aRecordorRow. \n",
      "SQL (Structured Query Language)\n",
      "13§StandardRelationalDatabaseLanguage§AllowstocreateDB§AllowstomanipulateDB§SupportedFeatures:•DataDefinitionLanguage•DataManipulationLanguage•DataControlLanguageWhat Can SQL do?\n",
      "14§execute queries against a database§retrieve data from a database§insert records in a database§update records in a database§delete records from a database§create new databases§create new tables in a databaseLecture 1 Data Types, Data Definition CommandsData Types in SQL\n",
      "16§char(n): Fixed length character string, with user-specified length n.§varchar(n) : Variable length character strings, with user-specified maximum length n.§Int : Integer (a finite subset of the integers that is machine-dependent).§number (p,d):  Fixed point number, with user-specified precision of p digits, with n digits to the right of decimal point. Eg: numeric(3,1) allows 44.5 to be stored exactly §float(n): Floating point number, with user-specified precision of at least n digits.Data Types in SQL….Continue…..§DATE:•It is used to store date and time information .•It stores century ,month ,day, hour minute and seconds.•If time is not specified then it stores the time of 12:00:00 at midnight if not the system time(default in 24 hour format)•If date information is not provided by default it takes date of first day of current month.§RAW: It stores binary data upto 2000 bytes long(256 bytes in 7.X)§LONG: It is used to store character data upto 2 G.B in size§LONG RAW: It is used to store binary data upto 2GB in size.17Data Types in SQL….Continue…..§CLOB: (Character Large Object):•It is used to store character strings upto 4 GB in size .There can be multiple CLOB columns per table.§BLOB (Binary Large Object):•It is used to store binary strings upto 44GB in size.There can be multiple BLOB columns per table.§BFILE: It stores the pointer  or locator pointing towards operating system.§NCLOB: N(Nationa)CLOB:-•   It is used to store multibyte character.      Eg:In English one character uses 1 byte18SQL Statements\n",
      "19SELECT     INSERTUPDATEDELETEMERGECREATEALTERDROPRENAMETRUNCATECOMMITROLLBACKSAVEPOINTGRANTREVOKEData retrieval Data manipulation language (DML)Data definition language (DDL)Transaction controlData control language (DCL)§CREATE§ALTER§DROP §TRUNCATE§RENAME\n",
      "20DATA DEFINITION STATEMENTSNaming Rules§Table names and column names:•Must begin with a letter•Must be 1–30 characters long•Must contain only A–Z, a–z, 0–9, _, $, and #•Must not duplicate the name of another object owned by the same user•Must not be an Oracle server reserved wordCREATE TABLE§Create TableCREATE TABLE <tablename> ( columnname1 datatype (size),   columnname2 datatype (size),   columnname3 datatype (size));\n",
      "Table created.§Create the table.Creating TablesCREATE TABLE dept (deptno NUMERIC(2),  dname VARCHAR(14),  loc VARCHAR(13));\n",
      "Table created.DESCRIBE dept\n",
      "§Confirm table creation.The ALTER TABLE StatementUse the ALTER TABLE statement to:•Add a new column•Modify an existing column•Define a default value for the new column•Drop a columnThe ALTER TABLE StatementUse the ALTER TABLE statement to add, modify, or drop columns.ALTER TABLE tableADD     (column datatype [DEFAULT expr]     [, column datatype]...);ALTER TABLE tableMODIFY    (column datatype [DEFAULT expr]     [, column datatype]...);ALTER TABLE tableDROP       (column);Adding a ColumnDEPT80“Add a new column to the DEPT80 table.”DEPT80New column\n",
      "Adding a Column•You use the ADD clause to add columns.\n",
      "•The new column becomes the last column.ALTER TABLE dept80ADD     (job_id VARCHAR(9));\n",
      "Table altered.\n",
      "Modifying a Column•You can change a column’s data type, size, and default value.\n",
      "•A change to the default value affects only subsequent insertions to the table.ALTER TABLE dept80MODIFY  (last_name VARCHAR(30));\n",
      "Table altered.Dropping a ColumnUse the DROP COLUMN clause to drop columns you no longer need from the table.ALTER TABLE  dept80DROP COLUMN  job_id; \n",
      "Table altered.Dropping a Table•All data and structure in the table is deleted.DROP TABLE dept80;\n",
      "Table dropped.Lecture 2 Data Manipulation Commands§ INSERT§ UPDATE§ DELETEDATA MANIPULATION STATEMENTSINSERT•Insert  into Table\n",
      "If datatype is varchar & date give single quotation mark while inserting data in a tableINSERT INTO <tablename> VALUES ( VALUE1,’VALUE2’,..);\n",
      "1 ROW AFFECTED.INSERT INTO <tablename> VALUES ( &COLUMNNAME1,’&COLUMNNAME’,..);\n",
      "1 ROW AFFECTED.\n",
      "SQL>/            TO ENTER MORE VALUESINSERT•Insert  into TableINSERT INTO EMPLOYEE VALUES ( 100,’JOHN’,10000);\n",
      "1 ROW AFFECTED.INSERT INTO EMPLOYEE VALUES ( &EMP_ID,’&EMP_NAME’,&SALARY);\n",
      "Enter Emp_id:100\n",
      "Enter Emp_name:John\n",
      "Enter Salary:10000\n",
      "1 ROW AFFECTED.\n",
      "SQL>/            TO ENTER MORE VALUESUPDATE•Update values of tableUPDATE <tablename> SET COLUMNNAME = NEWVALUE WHERE <CONDITION>; UPDATE EMPLOYEE SET EMP_ID = 80 WHERE EMP_NAME=‘JOHN’;DELETE•Delete entire rows from table\n",
      "•Delete particular row from tableDELETE FROM <TABLENAME>;DELETE FROM EMPLOYEE; DELETE  FROM TABLENAME WHERE <CONDITION>;   DELETE FROM EMPLOYEE WHERE DEPT_NO=10;Lecture 2ConstraintsWhat are Constraints?§Constraints enforce rules at the table level.§Constraints ensures that changes made to a database do not result in a loss of data consistency.§Types of Constraints :•NOT NULL•UNIQUE •PRIMARY KEY•FOREIGN KEY (Referential Integrity)•CHECKConstraint Guidelines§Create a constraint either:•At the same time as the table is created, or•After the table has been created§Define a constraint at the column or table level.§View a constraint in the data dictionary.Defining ConstraintsCREATE TABLE <tablename>     (column1 datatype [CONSTRAINT TYPE],   column2 datatype               CONSTRAINT [CONSTRAINT VARIABLE]                [CONSTRAINT TYPE]               ...  );CREATE TABLE employees(       employee_id  NUMBER(6),         first_name   VARCHAR2(20),       ...       job_id       VARCHAR2(10) NOT NULL,      CONSTRAINT emp_emp_id_pk              PRIMARY KEY (EMPLOYEE_ID));The NOT NULL ConstraintEnsures that null values are not permitted for the column:\n",
      "NOT NULL constraint(No row can containa null value forthis column.)Absence of NOT NULL constraint(Any row can containnull for this column.)NOT NULL constraint\n",
      "…CREATE TABLE employees(    employee_id    NUMBER(6),    last_name      VARCHAR2(25) NOT NULL,    salary         NUMBER(8,2),    commission_pct NUMBER(2,2),    hire_date      DATE                    CONSTRAINT emp_hire_date_nn                   NOT NULL,...  The NOT NULL ConstraintIs defined at the column level:System namedUsernamedThe UNIQUE ConstraintEMPLOYEES UNIQUE constraint\n",
      "INSERT INTONot allowed: already existsAllowed\n",
      "…The UNIQUE Constraint§Defined at either the table level or the column level: CREATE TABLE employees(    employee_id      NUMBER(6),    last_name        VARCHAR2(25) NOT NULL,    email            VARCHAR2(25),    salary           NUMBER(8,2),    commission_pct   NUMBER(2,2),    hire_date        DATE NOT NULL,...      CONSTRAINT emp_email_uk UNIQUE(email));The PRIMARY KEY ConstraintDEPARTMENTS PRIMARY KEY\n",
      "INSERT INTONot allowed(Null value)Not allowed (50 already exists)\n",
      "…CREATE TABLE   departments(    department_id        NUMBER(4),    department_name      VARCHAR2(30)       CONSTRAINT dept_name_nn NOT NULL,    manager_id           NUMBER(6),    location_id          NUMBER(4),      CONSTRAINT dept_id_pk PRIMARY KEY(department_id));The PRIMARY KEY Constraint§Defined at either the table level or the column level:The FOREIGN KEY ConstraintDEPARTMENTS EMPLOYEESFOREIGNKEYINSERT INTONot allowed(9 does not exist)\n",
      "AllowedPRIMARYKEY\n",
      "…\n",
      "…\n",
      "The FOREIGN KEY Constraint§Defined at either the table level or the column level:CREATE TABLE employees(    employee_id      NUMBER(6),    last_name        VARCHAR2(25) NOT NULL,    email            VARCHAR2(25),    salary           NUMBER(8,2),    commission_pct   NUMBER(2,2),    hire_date        DATE NOT NULL,...    department_id    NUMBER(4),    CONSTRAINT emp_dept_fk FOREIGN KEY (department_id)      REFERENCES departments(department_id),    CONSTRAINT emp_email_uk UNIQUE(email));FOREIGN KEY Constraint Keywords§FOREIGN KEY: Defines the column in the child table at the table constraint level§REFERENCES: Identifies the table and column in the parent table§ON DELETE CASCADE: Deletes the dependent rows in the child table when a row in the parent table is deleted.§ON DELETE SET NULL: Converts dependent foreign key values to nullCascading Actions in Referential Integrity\n",
      "50create table course (    course_id char(5) primary key,    title             varchar(20), dept_name varchar(20) references department)create table course (    …    dept_name varchar(20),    foreign key (dept_name) references department                on delete cascade                on update cascade,    . . . )alternative actions to cascade:  set null, set defaultThe CHECK Constraint§Defines a condition that each row must satisfy..., salary NUMBER(2)     CONSTRAINT emp_salary_min              CHECK (salary > 0),...Adding a Constraint Syntax§Use the ALTER TABLE statement to:•Add or drop a constraint, but not modify its structure•Enable or disable constraints•Add a NOT NULL constraint by using the MODIFY clauseALTER TABLE  table  ADD [CONSTRAINT constraint] type (column);Adding a ConstraintAdd a FOREIGN KEY constraint to the EMPLOYEES table indicating that a manager must already exist as a valid employee in the EMPLOYEES table.ALTER TABLE     employeesADD CONSTRAINT  emp_manager_fk   FOREIGN KEY(manager_id)   REFERENCES employees(employee_id);\n",
      "Table altered.Dropping a Constraint§Remove the manager constraint from the EMPLOYEES table.\n",
      "ALTER TABLE      employeesDROP CONSTRAINT  emp_manager_fk;\n",
      "Table altered.References\n",
      "55•Dr. P. S. Deshpande, SQL and PL/SQL for Oracle 10g, Black Book, Dreamteach Press.•Korth, Silberchatz, Sudarshan, “Database System Concepts”, 6th Edition McGraw –Hill.Data Management & Visualization (DMV) Module 1Structured Query Language (SQL)Index\n",
      "•57Lecture  3SELECT statement; ClausesLecture  3Aggregate Functions- Group by, having Lecture  4Views in SQLLecture  4Nested and complex queriesLecture 3 String OperationsBasic SELECT StatementSELECT columnname1,columnname2,…FROM table;•SELECT identifies what columns•FROM identifies which tableSELECT *FROM   departments;Selecting All Columns\n",
      "Selecting Specific ColumnsSELECT department_id, location_idFROM   departments;\n",
      "Writing SQL Statements§SQL statements are not case sensitive. §SQL statements can be on one or more lines.§Keywords cannot be abbreviated or split across lines.§Indents are used to enhance readability.Arithmetic ExpressionsCreate expressions with number and date data by using arithmetic operators.Operator+-*      /       DescriptionAddSubtract Multiply DivideUsing Arithmetic OperatorsSELECT last_name, salary, salary + 300FROM   employees;…\n",
      "Operator Precedence§Multiplication and division take priority over addition and subtraction.§Operators of the same priority are evaluated from left to right.§Parentheses are used to force prioritized evaluation and to clarify statements.\n",
      "*\n",
      "/\n",
      "+\n",
      "_Operator PrecedenceSELECT last_name, salary, 12*salary+100FROM   employees;…\n",
      "Using ParenthesesSELECT last_name, salary, 12*(salary+100)FROM   employees;…\n",
      "Limiting Rows Using a Selection\n",
      "“retrieve allemployeesin department 90”EMPLOYEES…\n",
      "Limiting the Rows Selected§Restrict the rows returned by using the WHERE clause.\n",
      "§The WHERE clause follows the FROM clause.SELECT *|columnname1, columnname2,..FROM table[WHERE condition(s)];Using the WHERE ClauseSELECT employee_id, last_name, job_id, department_idFROM   employeesWHERE  department_id = 90 ;\n",
      "Comparison ConditionsOperator=>      >= <      <= <>MeaningEqual toGreater than Greater than or equal to Less than Less than or equal toNot equal toSELECT last_name, salaryFROM   employeesWHERE  salary <= 3000;Using Comparison Conditions\n",
      "Other Comparison ConditionsOperatorBETWEEN...AND...IN(set)LIKEIS NULLMeaningBetween two values (inclusive), Match any of a list of values Match a character pattern Is a null value Using the BETWEEN Condition§Use the BETWEEN condition to display rows based on a range of values.SELECT last_name, salaryFROM   employeesWHERE  salary BETWEEN 2500 AND 3500;Lower limitUpper limit\n",
      "SELECT employee_id, last_name, salary, manager_idFROM   employeesWHERE  manager_id IN (100, 101, 201);Using the IN ConditionUse the IN membership condition to test for values in a list.\n",
      "Using the NULL Conditions§Test for nulls with the IS NULL operator.SELECT last_name, manager_idFROM   employeesWHERE  manager_id IS NULL;\n",
      "Logical ConditionsOperatorANDORNOTMeaningReturns TRUE if both component conditions are true Returns TRUE if either component condition is trueReturns TRUE if the following  condition is falseUsing the AND Operator§AND requires both conditions to be true.SELECT employee_id, last_name, job_id, salaryFROM   employeesWHERE  salary >=10000AND    job_id LIKE '%MAN%';\n",
      "Using the OR OperatorSELECT employee_id, last_name, job_id, salaryFROM   employeesWHERE  salary >= 10000OR     job_id LIKE '%MAN%';\n",
      "§OR requires either condition to be trueSELECT last_name, job_idFROM   employeesWHERE  job_id        NOT IN ('IT_PROG', 'ST_CLERK', 'SA_REP');Using the NOT Operator\n",
      "Rules of Precedence\n",
      "§Override rules of precedence by using parentheses.Order Evaluated Operator    1 Arithmetic operators  2 Concatenation operator 3 Comparison conditions 4 IS [NOT] NULL, LIKE, [NOT] IN 5 [NOT] BETWEEN 6 NOT logical condition 7 AND logical condition 8 OR logical conditionSELECT last_name, job_id, salaryFROM   employeesWHERE  job_id = 'SA_REP'OR     job_id = 'AD_PRES'AND    salary > 15000;Rules of Precedence\n",
      "SELECT last_name, job_id, salaryFROM   employeesWHERE  (job_id = 'SA_REP'OR     job_id = 'AD_PRES')AND    salary > 15000;Rules of Precedence§Use parentheses to force priority.\n",
      "Duplicate Rows§The default display of queries is all rows, including duplicate rows.SELECT department_idFROM   employees;…\n",
      "Eliminating Duplicate Rows§Eliminate duplicate rows by using the DISTINCT keyword in the SELECT clause.SELECT DISTINCT department_idFROM   employees;\n",
      "Aggregate Functions and Null ValuesLecture No: 3What Are Group Functions?§Group functions operate on sets of rows to give one result per group.EMPLOYEESThe maximum   salary in the EMPLOYEES table.\n",
      "…\n",
      "Types of Group Functions•AVG •COUNT •MAX•MIN •SUMSELECT [column,] group_function(column), ...FROM  table[WHERE condition][GROUP BY column][ORDER BY column];Group Functions SyntaxSELECT AVG(salary), MAX(salary),       MIN(salary), SUM(salary)FROM   employeesWHERE  job_id LIKE '%REP%';Using the AVG and SUM Functions§You can use AVG and SUM for numeric data.\n",
      "Using the MIN and MAX Functions§You can use MIN and MAX for any data type.SELECT MIN(hire_date), MAX(hire_date)FROM   employees;\n",
      "SELECT COUNT(*)FROM   employeesWHERE  department_id = 50;Using the COUNT Function§COUNT(*) returns the number of rows in a table.\n",
      "Using the COUNT Function§COUNT(expr) returns the number of rows with non-null values for the expr.§Display the number of department values in the EMPLOYEES table, excluding the null values.SELECT COUNT(commission_pct)FROM   employeesWHERE  department_id = 80;\n",
      "SELECT AVG(commission_pct)FROM   employees;Group Functions and Null Values§Group functions ignore null values in the column.\n",
      "SELECT AVG(NVL(commission_pct, 0))FROM   employees;Using the NVL Function with Group FunctionsThe NVL function forces group functions to include null values.\n",
      "Creating Groups of Data EMPLOYEESThe averagesalary in EMPLOYEEStable for each department.4400\n",
      "…\n",
      "95003500640010033SELECT column, group_function(column)FROM  table[WHERE condition][GROUP BY group_by_expression][ORDER BY column <DESCending order>];Creating Groups of Data: The GROUP BY Clause Syntax\n",
      "§Divide rows in a table into smaller groups by using the §GROUP BY clause.SELECT   department_id, AVG(salary)FROM     employeesGROUP BY department_id ;Using the GROUP BY Clause §All columns in the SELECT list that are not in group functions must be in the GROUP BY clause.\n",
      "Using the GROUP BY Clause §The GROUP BY column does not have to be in the SELECT list.SELECT   AVG(salary)FROM     employeesGROUP BY department_id ;\n",
      "Grouping by More Than One ColumnEMPLOYEES“Add up the salaries in the EMPLOYEES tablefor each job, grouped by department.\n",
      "…SELECT   dept_id, job_id, SUM(salary)FROM     employeesGROUP BY department_id, job_id ;Using the GROUP BY Clause on Multiple Columns\n",
      "Illegal Queries Using Group Functions§You cannot use the WHERE clause to restrict groups.§You use the HAVING clause to restrict groups.§You cannot use group functions in the WHERE clause.SELECT   department_id, AVG(salary)FROM     employeesWHERE    AVG(salary) > 8000GROUP BY department_id;WHERE  AVG(salary) > 8000       *ERROR at line 3:ORA-00934: group function is not allowed hereCannot use the WHERE clause to restrict groupsSELECT   last_name, job_id, department_id, hire_dateFROM     employeesORDER BY hire_date ;ORDER BY Clause§Sort rows with the ORDER BY clause§ASC: ascending order, default§DESC: descending order§The ORDER BY clause comes last in the SELECT statement.\n",
      "…\n",
      "Sorting in Descending OrderSELECT   last_name, job_id, department_id, hire_dateFROM     employeesORDER BY hire_date DESC ;…\n",
      "Excluding Group Results\n",
      "The maximumsalaryper departmentwhen it isgreater than$10,000EMPLOYEES\n",
      "…SELECT column, group_functionFROM  table[WHERE condition][GROUP BY group_by_expression][HAVING group_condition][ORDER BY column];Excluding Group Results: The HAVING ClauseUse the HAVING clause to restrict groups:§Rows are grouped.§The group function is applied.§Groups matching the HAVING clause are displayed.Using the HAVING ClauseSELECT   department_id, MAX(salary)FROM     employeesGROUP BY department_idHAVING   MAX(salary)>10000 ;\n",
      "SELECT   job_id, SUM(salary) PAYROLLFROM     employeesWHERE    job_id NOT LIKE '%REP%'GROUP BY job_idHAVING   SUM(salary) > 13000ORDER BY SUM(salary);Using the HAVING Clause\n",
      "Nesting Group Functions§Display the maximum average salary. SELECT   MAX(AVG(salary))FROM     employeesGROUP BY department_id;\n",
      "Views in SQLLecture No: 4What is a View?EMPLOYEES Table:\n",
      "Why Use Views?§To restrict data access§To make complex queries easy§To provide data independence§To present different views of the same dataCreating a View§You embed a subquery within the CREATE VIEW statement.\n",
      "§The subquery can contain complex SELECT syntax.CREATE VIEW <viewname> AS (subquery);Creating a View§Create a view, EMPVU80, that contains details of employees in department 80.\n",
      "§Describe the structure of the view by using the  DESCRIBE command.DESCRIBE empvu80CREATE VIEW empvu80 AS SELECT  employee_id, last_name, salary    FROM    employees    WHERE   department_id = 80;\n",
      "View created.Creating a View§Create a view by using column aliases in the subquery.\n",
      "§Select the columns from this view by the given alias names.CREATE VIEW salvu50 AS SELECT  employee_id ID_NUMBER, last_name NAME,            salary*12 ANN_SALARY    FROM    employees    WHERE   department_id = 50;\n",
      "View created.Retrieving Data from a ViewSELECT *FROM salvu50;\n",
      "Modifying a View§Modify the EMPVU80 view by using CREATE OR REPLACE VIEW clause. Add an alias for each column name.\n",
      "§Column aliases in the CREATE VIEW clause are listed in the same order as the columns in the subquery.CREATE OR REPLACE VIEW empvu80  (id_number, name, sal, department_id)AS SELECT  employee_id, first_name || ' ' || last_name,            salary, department_id   FROM    employees   WHERE   department_id = 80;\n",
      "View created.Creating a Complex View§Create a complex view that contains group functions to display values from two tables.CREATE VIEW dept_sum_vu  (name, minsal, maxsal, avgsal)AS SELECT  d.department_name, MIN(e.salary),              MAX(e.salary),AVG(e.salary)   FROM      employees e, departments d   WHERE     e.department_id = d.department_id    GROUP BY  d.department_name;\n",
      "View created.§You can ensure that DML operations performed on the view stay within the domain of the view by using the WITH CHECK OPTION clause.\n",
      "§ §Any attempt to change the department number for any row in the view fails because it violates the WITH CHECK OPTION constraint.CREATE OR REPLACE VIEW empvu20AS SELECT *   FROM     employees   WHERE    department_id = 20   WITH CHECK OPTION CONSTRAINT empvu20_ck ;\n",
      "View created.Using the WITH CHECK OPTION ClauseDenying DML Operations§You can ensure that no DML operations occur by adding the WITH READ ONLY option to your view definition.§Any attempt to perform a DML on any row in the view results in an Oracle server error.Denying DML OperationsCREATE OR REPLACE VIEW empvu10    (employee_number, employee_name, job_title)AS SELECT employee_id, last_name, job_id   FROM     employees   WHERE    department_id = 10   WITH READ ONLY;\n",
      "View created.Nested and Complex QueriesLecture No: 4Outline§Introduction                                             §Subqueries with the SELECT Statement§Subqueries with the INSERT Statement§Subqueries with the Update Statement§Subqueries with the DELETE Statement\n",
      "Lecture 28 – Nested and Complex Queries123§A Subquery or Inner query or Nested query is a query within another SQL query and embedded within the WHERE clause.§A subquery is used to return data that will be used in the main query as a condition to further restrict the data to be retrieved.§Subqueries can be used with the SELECT, INSERT, UPDATE, and DELETE statements along with the operators like =, <, >, >=, <=, IN, BETWEEN etc.Introduction\n",
      "Lecture 28 – Nested and Complex Queries124§Subqueries must be enclosed within parentheses.§A subquery can have only one column in the SELECT clause, unless multiple columns are in the main query for the subquery to compare its selected columns.§An ORDER BY cannot be used in a subquery, although the main query can use an ORDER BY. The GROUP BY can be used to perform the same function as the ORDER BY in a subquery.§Subqueries that return more than one row can only be used with multiple value operators, such as the IN operator.§A subquery cannot be immediately enclosed in a set function.§The BETWEEN operator cannot be used with a subquery; however, the BETWEEN operator can be used within the subquery.There are a few rules that subqueries must follow:\n",
      "125Lecture 28 – Nested and Complex Queries§Subqueries are most frequently used with the SELECT statement. The basic syntax is as follows:Subqueries with the SELECT Statement\n",
      "126\n",
      "Lecture 28 – Nested and Complex QueriesExample:§Consider the CUSTOMERS table having the following records:Subqueries with the SELECT Statement (Cont.)\n",
      "127\n",
      "§Now, let us check following subquery with SELECT statement:\n",
      "This would produce the following result:Lecture 28 – Nested and Complex Queries§Subqueries also can be used with INSERT statements. The INSERT statement uses the data returned from the subquery to insert into another table. The selected data in the subquery can be modified with any of the character, date or number functions.§The basic syntax is as follows:3. Subqueries with the INSERT Statement\n",
      "128\n",
      "Lecture 28 – Nested and Complex QueriesExample:§Consider a table CUSTOMERS_BKP with similar structure as CUSTOMERS table. Now to copy complete CUSTOMERS table into CUSTOMERS_BKP, following is the syntax:Subqueries with the INSERT Statement (Cont.)\n",
      "129\n",
      "Lecture 28 – Nested and Complex Queries§The subquery can be used in conjunction with the UPDATE statement. Either single or multiple columns in a table can be updated when using a subquery with the UPDATE statement.§The basic syntax is as follows:Subqueries with the UPDATE Statement\n",
      "130\n",
      "Lecture 28 – Nested and Complex QueriesExample:§Assuming, we have CUSTOMERS_BKP table available which is backup of CUSTOMERS table.§Following example updates SALARY by 0.25 times in CUSTOMERS table for all the customers whose AGE is greater than or equal to 27:Subqueries with the UPDATE Statement (Cont.)\n",
      "Lecture 24 – Nested Queries131\n",
      "This would impact two rows and finally CUSTOMERS table would have the following records:Example:§Assuming, we have CUSTOMERS_BKP table available which is backup of CUSTOMERS table.§Following example deletes records from CUSTOMERS table for all the customers whose AGE is greater than or equal to 27:Subqueries with the DELETE Statement (Cont.) \n",
      "Lecture 24 – Nested Queries132\n",
      "This would impact two rows and finally CUSTOMERS table would have the following records:§The subquery can be used in conjunction with the DELETE statement like with any other statements mentioned above.§The basic syntax is as follows:Subqueries with the DELETE Statement\n",
      "133\n",
      "Lecture 28 – Nested and Complex Queries§Derived Relations§The with ClauseIntroduction\n",
      "134Lecture 28 – Nested and Complex Queries§Find the average account balance of those branches where the average account balance is greater than $1,000.select bname, avg-balance from (select bname, avg(balance) from account group by bname) as result(bname, avg-balance) where avg-balance > 1000 Derived Relations\n",
      "135Lecture 28 – Nested and Complex Queries§Consider the following query, which selects accounts with the maximum balance ; if there are many accounts with the same maximum balance, all of them are selected. with Max_balance (Value) as  select  max (Balance)  from Account select Account_number from Account, Max_balance where Account.Balance= Max_balance.Value;With Clause\n",
      "136Lecture 28 – Nested and Complex QueriesReferences\n",
      "137•Dr. P. S. Deshpande, SQL and PL/SQL for Oracle 10g, Black Book, Dreamteach Press.•Korth, Silberchatz, Sudarshan, “Database System Concepts”, 6th Edition McGraw –Hill.Subject Name:Unit No:2   Data Management and VisualizationUnit Name:Data Warehousing: Concepts, Architecture, and Implementation What is Data Warehouse ?\n",
      "139\n",
      "139Module 2: Data Warehousing: Concepts, Architecture, and Implementation Why do we need Data Warehouse ?\n",
      "140\n",
      "140Module 2: Data Warehousing: Concepts, Architecture, and Implementation Why is Data Warehouse so important?\n",
      "141\n",
      "141Module 2: Data Warehousing: Concepts, Architecture, and Implementation Why is Data Warehouse so important? Cont..\n",
      "142\n",
      "142Module 2: Data Warehousing: Concepts, Architecture, and Implementation Functional definition of Data WarehouseThe data warehouse is an informational environment that:§Provides an integrated and total view of the enterprise§Makes the enterprise’s current and historical information easily available for decision making§Makes decision-support transactions possible without hindering operational systems§Renders the organization’s information consistent§Presents a flexible and interactive source of strategic informationBill Inmon, considered to be the father of Data Warehousing provides the following definition:§ “A Data Warehouse is a subject oriented, integrated, nonvolatile, and time variant collection of data in support of management’s decisions.”\n",
      "Module 2: Data Warehousing: Concepts, Architecture, and Implementation 143Features of Data Warehousing§Sean Kelly, data warehouse practitioner, defines the data warehousing in following way. The data in data warehousing is:§Subject oriented§Integrated§Time stamped§Non-volatile \n",
      "Module 2: Data Warehousing: Concepts, Architecture, and Implementation 144Features of Data Warehousing – subject oriented data\n",
      "Module 2: Data Warehousing: Concepts, Architecture, and Implementation 145\n",
      "Features of Data Warehousing – integrated data\n",
      "Module 2: Data Warehousing: Concepts, Architecture, and Implementation 146\n",
      "Features of Data Warehousing – integrated data§Before the data from various disparate sources can be usefully stored in a data warehouse, you have to:§remove the inconsistencies;§standardize the various data elements;§make sure of the meanings of data names in each source application §Before moving the data into the data warehouse, you have to go through a process of transformation, consolidation, and integration of the source data§Here are some of the items that would need standardization:§Naming conventions§Codes§Data attributes§MeasurementsModule 2: Data Warehousing: Concepts, Architecture, and Implementation 147Features of Data Warehousing – Time Variant§For an operational system, the stored data contains the current values.§The data in the data warehouse is meant for analysis and decision making.§A data warehouse, because of the vary nature of its purpose, has to contain historical data, not just current values§Data is stored as snapshots over past and current periods§Every data structure in the data warehouse contains the time element§The time variant nature of data in a data warehouse§Allows for analysis of the past§Relates information to the present§Enables forecast for the futureModule 2: Data Warehousing: Concepts, Architecture, and Implementation 148Features of Data Warehousing – non volatile data\n",
      "Module 2: Data Warehousing: Concepts, Architecture, and Implementation 149\n",
      "Data Warehouse Architecture\n",
      "150\n",
      "150Module 2: Data Warehousing: Concepts, Architecture, and Implementation © 2019 Snowflake Inc. All Rights ReservedLEGACY DATA LANDSCAPE\n",
      "151EDWData Sources\n",
      "Data Lakeor HadoopDatamartsETLBI / AnalyticsOLTP DatabasesEnterprise Applications\n",
      "Web AppsThird-Party\n",
      "Other© 2019 Snowflake Inc. All Rights ReservedMODERN DATA LANDSCAPE\n",
      "152OLTP DatabasesEnterprise Applications\n",
      "Web AppsThird-Party\n",
      "Other\n",
      "EDWData SourcesLogical DatamartsETL or ETLBI / Analytics\n",
      "Data Lakeor HadoopData Warehouse ArchitectureThere are 3 approaches for constructing Data Warehouse layers: Single-tier architectureThe objective of a single layer is to minimize the amount of data stored. This goal is to remove data redundancy. This architecture is not frequently used in practice.Two-tier architectureIt separates physically available sources and data warehouse.Not expandable and also not supporting a large number of end-users. It also has connectivity problems because of network limitations.Three-Tier Data Warehouse ArchitectureThis is the most widely used Architecture of Data Warehouse.153Module 2: Data Warehousing: Concepts, Architecture, and Implementation 3 Tier Data Warehouse Architecture\n",
      "154\n",
      "154Module 2: Data Warehousing: Concepts, Architecture, and Implementation 3 Tier Data Warehouse Architecture\n",
      "155Generally a data warehouses adopts a three-tier architecture. Bottom Tier − The bottom tier of the architecture is the data warehouse database server. It is the relational database system. We use the back end tools and utilities to feed data into the bottom tier. These back end tools and utilities perform the Extract, Clean, Load, and refresh functions.Middle Tier − In the middle tier, we have the OLAP Server that can be implemented in either of the following ways.lBy Relational OLAP (ROLAP), which is an extended relational database management system. The ROLAP maps the operations on multidimensional data to standard relational operations.lBy Multidimensional OLAP (MOLAP) model, which directly implements the multidimensional data and operations.Top-Tier − This tier is the front-end client layer. This layer holds the query tools, reporting tools, analysis tools and data mining tools.155Module 2: Data Warehousing: Concepts, Architecture, and Implementation Data Mart\n",
      "156\n",
      "Data mart contains a subset of organization-wide data. This subset of data is valuable to specific groups of an organization.In other words, we can claim that data marts contain data specific to a particular group. For example, the marketing data mart may contain data related to items, customers, and sales. Data marts are confined to subjects.\n",
      "156Module 2: Data Warehousing: Concepts, Architecture, and Implementation Data Mart\n",
      "157Points to remember about data marts −lUnix/Linux-based servers are used to implement data marts. lThey are implemented on low-cost servers.lThe implementation data mart cycles is measured in short periods of time, i.e., in weeks rather than months or years.lData marts are small in size.lData marts are customized by department.lThe source of a data mart is departmentally structured data warehouse.lData mart are flexible.157Module 2: Data Warehousing: Concepts, Architecture, and Implementation Data warehouse vs Data Mart\n",
      "158lData Mart is smaller version of Data Warehouse which deals with single subjectlData marts are focused on one area, hence they draw data from limited number of sourceslTime taken to build data mart is very less compared to DWH\n",
      "158Module 2: Data Warehousing: Concepts, Architecture, and Implementation Types of Data Mart\n",
      "159lDependent Data Mart: Data comes from OLTP source to Data Warehouse and then from data warehouse to Data MartlIndependent Data Mart: Data directly received from the source system, This is suitable for small organizationlHybrid Data Mart: Data fed from both OLTP source and DWH \n",
      "159Module 2: Data Warehousing: Concepts, Architecture, and Implementation Data Warehouse Design Approaches:Top-Down and Bottom-Up\n",
      "160lData Warehouse design approaches are very important aspect of building data warehouse. lSelection of right data warehouse design could save lot of time and project cost.lThere are two different Data Warehouse Design Approaches normally followed when designing a Data Warehouse solution and based on the requirements of your project you can choose which one suits your particular scenario.\n",
      "160Module 2: Data Warehousing: Concepts, Architecture, and Implementation Top-Down Approach for Data warehouse Design\n",
      "161lIn the top-down approach, the data warehouse is designed first and then data mart are builtlBelow are the steps that are involved in top-down approach:lData is extracted from the various source systems using ETL tools, it is validated and pushed to the data warehouse.lYou will apply various aggregation, summerization techniques on extracted data from data warehouse and loaded back to the data warehouselOnce the aggregation and summerization is completed, various data marts extract that data and apply the some more transformation to make the data structure as defined by the data marts.lThis is bill inmons methodology\n",
      "161Module 2: Data Warehousing: Concepts, Architecture, and Implementation Bottom-up Approach for Data warehouse Design\n",
      "162lRalph Kimball proposed data warehouse design approach is called dimensional modelling or the Kimball methodology. lThis methodology follows the bottom-up approachlAs per this method, data marts are first created to provide the reporting and analytics capability for specific business processlLater with these data marts, enterprise data warehouse is created\n",
      "162Module 2: Data Warehousing: Concepts, Architecture, and Implementation Dimensional Modelling\n",
      "163What is Dimensional Model?lA dimensional model is a data structure technique optimized for Data warehousing tools. lThe concept of Dimensional Modelling was developed by Ralph Kimball and is comprised of \"fact\" and \"dimension\" tables.lA Dimensional model is designed to read, summarize, analyze numeric information like values, balances, counts, weights, etc. in a data warehouse. lIn contrast, relational models are optimized for addition, updating and deletion of data in a real-time Online Transaction System.lER modeling is for reducing redundancy of data, where as dimensional model arranges data in such a way that it is easier to retrieve information and generate reportslThese dimensional and relational models have their unique way of data storage that has specific advantages.lDimensional models are used in data warehouse systems and not a good fit for relational systems163Module 2: Data Warehousing: Concepts, Architecture, and Implementation Elements of Dimensional Data ModelFactFacts are the measurements/metrics or facts from your business process. For a Sales business process, a measurement would be quarterly sales numberDimensionDimension provides the context surrounding a business process event. In simple terms, they give who, what, where of a fact. In the Sales business process, for the fact quarterly sales number, dimensions would be•Who – Customer Names•Where – Location•What – Product NameIn other words, a dimension is a window to view information in the facts.164Module 2: Data Warehousing: Concepts, Architecture, and Implementation Elements of Dimensional Data ModelAttributesThe Attributes are the various characteristics of the dimension in dimensional data modeling.In the Location dimension, the attributes can be•State•Country•Zipcode etc.Attributes are used to search, filter, or classify facts. Dimension Tables contain Attributes165Module 2: Data Warehousing: Concepts, Architecture, and Implementation What is Fact Table?•A Fact table stores quantified data to measure the business performance.•It is a measure that can be summed, averaged or manipulated. •Fact table is a table surrounded by the dimension tables in the Star Schema of Data Warehouse.The Fact table consists of two types of column:•A Dimension key (foreign key)– A foreign key that joins with dimension tables•A Measure– where data is analyzed•A dimension table is a table in a star schema of a data warehouse. •A dimension table stores attributes, or dimensions, that describe the objects in a fact table.166Module 2: Data Warehousing: Concepts, Architecture, and Implementation Fact and Dimensional Table \n",
      "167Module 2: Data Warehousing: Concepts, Architecture, and Implementation ER Modelling vs Dimensional Modelling\n",
      "168ER ModelingDimensional ModelingData Stored in RDBMSData Stored in RDBMS or Multidimensional databasesTables are unit of storageCubes are the unit of storageData is normalized and used for OLTPData is de normalized and used for data warehouse and data martsSeveral tables and chain of relationship between themFew facts tables are connected to several dimension tablesVolatile(frequent updates)Non volatileTime variantTime invariantDetailed level of transaction dataSummary of bulky transaction data (Aggregations and measures) are used in business decisionsSQL is used  to manipulate the dataSQL or MDX are used to manipulate the dataNormal reportsInteractive reports, user friendly, drag and drop MD OLAP reports168Module 2: Data Warehousing: Concepts, Architecture, and Implementation •In several ways, building a data warehouse is very different from building an operational system.•This becomes notable especially in the requirements gathering phase. •Because of this difference, the traditional methods of collecting requirements that work well for operational systems cannot be applied to data warehouses.DEFINING THE BUSINESS REQUIREMENTS\n",
      "169169Module 2: Data Warehousing: Concepts, Architecture, and Implementation •In data warehousing system, the users are generally unable to define their requirements clearly. •Users cannot define precisely what information they really want from the data warehouse, nor can they express how they would like to use the information or process it.•Managers think of the business in terms of business dimensions.•If your users of the data warehouse think in terms of business dimensions for decision making, you should also think of business dimensions while collecting requirements.Dimensional Nature of Business Data\n",
      "170\n",
      "170Module 2: Data Warehousing: Concepts, Architecture, and Implementation Dimension Hierarchies/Categories•When a user analyzes the measurements along a business dimension, the user usually would like to see the numbers first in summary and then at various levels of detail. •What the user does here is to traverse the hierarchical levels of a business dimension for getting the details at various levels. •The hierarchy of the time dimension consists of the levels of year, quarter, and month. •The dimension hierarchies are the paths for drilling down or rolling up in our analysis.•Within each major business dimension there are categories of data elements that can also be useful for analysis.•Hierarchies and categories are included in the information packages for each dimension.\n",
      "171Module 2: Data Warehousing: Concepts, Architecture, and Implementation lDimensionalDataModelingisoneofthedatamodelingtechniquesusedindatawarehousedesign.lGoal:ImprovethedataretrievallTheconceptofDimensionalModelingwasdevelopedbyRalphKimballwhichiscomprisedoffactsanddimensiontableslSincethemaingoalofthismodelingistoimprovethedataretrievalsoitisoptimizedforSELECTOPERATIONlTheadvantageofusingthismodelisthatwecanstoredatainsuchawaythatitiseasiertostoreandretrievethedataoncestoredinadatawarehouse.lDimensionalmodelisthedatamodelusedbymanyOLAPsystems.Dimensional Data Modeling \n",
      "172172Module 2: Data Warehousing: Concepts, Architecture, and Implementation Dimensional Data Modeling \n",
      "173\n",
      "Steps to Create Dimensional Data Modeling:Step-1: Identifying the business objective –The first step is to identify the business objective. Sales, HR, Marketing, etc. are some examples as per the need of the organization. Since it is the most important step of Data Modelling the selection of business objective also depends on the quality of data available for that process.Step-2: Identifying Granularity –Granularity is the lowest level of information stored in the table. The level of detail for business problem and its solution is described by Grain.173Module 2: Data Warehousing: Concepts, Architecture, and Implementation Dimensional Data Modeling \n",
      "174\n",
      "Step-3: Identifying Dimensions and its Attributes –Dimensions are objects or things like table. Dimensions categorize and describe data warehouse facts and measures in a way that support meaningful answers to business questions. A data warehouse organizes descriptive attributes as columns in dimension tables. For Example, the data dimension may contain data like a year, month and weekday.Step-4: Identifying the Fact –The measurable data is hold by the fact table. Most of the fact table rows are numerical values like price or cost per unit, etc.Step-5: Building of Schema –We implement the Dimension Model in this step. A schema is a database structure. Popular schemes: Star Schema, Snowflake Schema, Fact constellation scheme174Module 2: Data Warehousing: Concepts, Architecture, and Implementation Dimensions\n",
      "175\n",
      "End Users fires a queries on these tables which contains descriptive information175Module 2: Data Warehousing: Concepts, Architecture, and Implementation Facts and Measures\n",
      "176\n",
      "176Module 2: Data Warehousing: Concepts, Architecture, and Implementation Schema\n",
      "177\n",
      "177Module 2: Data Warehousing: Concepts, Architecture, and Implementation Star Schema in Data Warehouse modeling\n",
      "178Star schema is the fundamental schema among the data mart schema and it is simplest. This schema is widely used to develop or build a data warehouse and dimensional data marts. It includes one or more fact tables indexing any number of dimensional tables. The star schema is a necessary case of the snowflake schema. It is also efficient for handling basic queries.It is said to be star as its physical model resembles to the star shape having a fact table at its center and the dimension tables at its peripheral representing the star’s points. \n",
      "178Module 2: Data Warehousing: Concepts, Architecture, and Implementation Star Schema in Data Warehouse modeling\n",
      "179In the above demonstration, lSALES is a fact table having attributes i.e. (Product ID, Order ID, Customer ID, Employer ID, Total, Quantity, Discount) which references to the dimension tables(first 4) and next 3 are measures. lEmployee dimension table contains the attributes: Emp ID, Emp Name, Title, Department and Region. lProduct dimension table contains the attributes: Product ID, Product Name, Product Category, Unit Price. lCustomer dimension table contains the attributes: Customer ID, Customer Name, Address, City, Zip. lTime dimension table contains the attributes: Order ID, Order Date, Year, Quarter, Month.179Module 2: Data Warehousing: Concepts, Architecture, and Implementation Star Schema in Data Warehouse modeling\n",
      "180Model of Star Schema –In Star Schema, Business process data, that holds the quantitative data about a business is distributed in fact tables, and dimensions which are descriptive characteristics related to fact data. Sales price, sale quantity, distance, speed, weight, and weight measurements are few examples of fact data in star schema.\n",
      "180Module 2: Data Warehousing: Concepts, Architecture, and Implementation Star Schema in Data Warehouse modeling\n",
      "181Advantages of Star Schema –Simpler Queries:Join logic of star schema is quite cinch in compare to other join logic which are needed to fetch data from a transactional schema that is highly normalized.Simplified Business Reporting Logic:In compared to a transactional schema that is highly normalized, the star schema makes simpler common business reporting logic, such as as-of reporting and period-over-period.Feeding Cubes:Star schema is widely used by all OLAP systems to design OLAP cubes efficiently. In fact, major OLAP systems deliver a ROLAP mode of operation which can use a star schema as a source without designing a cube structure.\n",
      "181Module 2: Data Warehousing: Concepts, Architecture, and Implementation Star Schema in Data Warehouse modeling\n",
      "182Disadvantages of Star Schema –lData integrity is not enforced well since in a highly de-normalized schema state.lNot flexible in terms if analytical needs as a normalized data model.lStar schemas don’t reinforce many-to-many relationships within business entities – at least not frequently.\n",
      "182Module 2: Data Warehousing: Concepts, Architecture, and Implementation Snowflake Schema in Data Warehouse Model\n",
      "183●The snowflake schema is a variant of the star schema. ●Here, the centralized fact table is connected to multiple dimensions. ●In the snowflake schema, dimension are present in a normalized from in multiple related tables. ●The snowflake structure materialized when the dimensions of a star schema are detailed and highly structured, having several levels of relationship, and the child tables have multiple parent table. ●The snowflake effect affects only the dimension tables and does not affect the fact tables\n",
      "183Module 2: Data Warehousing: Concepts, Architecture, and Implementation 183Snowflake Schema in Data Warehouse Model\n",
      "184●The snowflake schema is a variant of the star schema. ●Here, the centralized fact table is connected to multiple dimensions. ●In the snowflake schema, dimension are present in a normalized from in multiple related tables. ●The snowflake structure materialized when the dimensions of a star schema are detailed and highly structured, having several levels of relationship, and the child tables have multiple parent table. ●The snowflake effect affects only the dimension tables and does not affect the fact tables\n",
      "184184Module 2: Data Warehousing: Concepts, Architecture, and Implementation Snowflake Schema in Data Warehouse Model\n",
      "185\n",
      "185eModule 2: Data Warehousing: Concepts, Architecture, and Implementation 185Snowflake Schema in Data Warehouse Model\n",
      "186●The Employee dimension table now contains the attributes: EmployeeID, EmployeeName, DepartmentID, Region, Territory. ●The DepartmentID attribute links with Employee table with the Department dimension table. ●The Department dimension is used to provide detail about each department, such as Name and Location of the department. ●The Customer dimension table now contains the attributes: CustomerID, CustomerName, Address, CityID. ●The CityID attributes links the Customer dimension table with the City dimension table. ●The City dimension table has details about each city such as CityName, Zipcode, State and Country.186Module 2: Data Warehousing: Concepts, Architecture, and Implementation 186Snowflake Schema in Data Warehouse Model\n",
      "187●The main difference between star schema and snowflake schema is that the dimension table of the snowflake schema are maintained in normalized form to reduce redundancy.●The advantage here is that such table(normalized) are easy to maintain and save storage space. ●However, it also means that more joins will be needed to execute query. This will adversely impact system performance.\n",
      "187Module 2: Data Warehousing: Concepts, Architecture, and Implementation 187Snowflake Schema in Data Warehouse Model\n",
      "188Advantages: There are two main advantages of snowflake schema given below:●It provides structured data which reduces the problem of data integrity.●It uses small disk space because data are highly structured.Disadvantages:●Snowflaking reduces space consumed by dimension tables, but compared with the entire data warehouse the saving is usually insignificant.●Avoid snowflaking or normalization of a dimension table, unless required and appropriate.●Do not snowflake hierarchies of one dimension table into separate tables. Hierarchies should belong to the dimension table only and should never be snowfalked.●Multiple hierarchies can belong to the same dimension has been designed at the lowest possible detail.188Module 2: Data Warehousing: Concepts, Architecture, and Implementation 188Factless Fact Table •A fact table without any measures is known as factless fact table. It’s basically an intersection of dimension.•The concept of factless fact table does not make sense & seems to be of not much use because of fact table. Essentially is all about facts, & there are no facts in a factless fact table•However there are circumstances where using a factless fact table makes sense in data ware housing.•Factless fact table provide flexibility in data warehouse design.•It contains many-many relationships between dimensions.•These table do not contain numeric textual facts189Module 2: Data Warehousing: Concepts, Architecture, and Implementation 189Commonly used Examples•In  tables such as keeping the attendance record students.•Identifying product promotion events•Tracking attendance of students or registration events.•Tracking insurance related accident events.\n",
      "190Module 2: Data Warehousing: 190Example \n",
      "•Think about a record of student attendance in classes.•In this case, the fact table would consist of 3 dimensions: the student dimension, the time dimension, and the class dimension.•This factless fact table would look like the following:For example, one can easily answer the following questions with this factless fact table:• How many students attended a particular class on a particular day?• How many classes on average does a student attend on a given day?Without using a factless fact table, we will need two separate fact tables to answer the above two questions.191Module 2: Data Warehousing: Concepts, Architecture, and Implementation 191Types of Fact TableThere are two types:-•Factless fact table for events.•Factless fact table for conditions.\n",
      "192Module 2: Data Warehousing: Concepts, Architecture, and Implementation 192Factless fact table for events.•Factless fact table for events is a table that records events. •In dimensional data warehouse, numerous event-tracking tables appear to be factless sometimes.•There may be a situation where no fact seems to be related to an important business process & you may have events that you want to track, but you cannot find any measurements. In such situations, create a typical transact-grained fact table that comprises no facts.\n",
      "193Module 2: Data Warehousing: Concepts, Architecture, and Implementation 193Factless fact table for conditions:-•If there are no clear transactions, factless fact tables are used to design the conditions or other important relationships among the different dimensions.•A factless fact table helps in creating analysis reports that comprises negative aspects of a business. For e.g. Book store that did not sell a single book for a given period.\n",
      "194194Module 2: Data Warehousing: Concepts, Architecture, and Implementation Evolution of Data Warehousing•1970s: The Foundations •Conceptual Emergence: The term \"data warehouse\" was coined by Bill Inmon, who is often referred to as the \"father of data warehousing\". •Mainframes: Early data warehouses relied heavily on mainframes and were mainly focused on centralizing data. •1980s: Growth and Refinement •Relational Databases: Introduction of relational databases provided a foundation for scaling data. •OLAP (Online Analytical Processing): Introduced by Dr. Edgar F. Codd, it allowed for complex analytical and ad-hoc queries with rapid execution. Evolution of Data Warehousing•1990s: Commercial Emergence •Popularization: The idea of data warehousing gained traction in enterprises. •ETL (Extract, Transform, Load): Emerged as a key process to load data into the warehouse. •Vendors Emerge: Major players like IBM, Oracle, and Teradata began offering data warehousing solutions. •Data Marts: Subsets of data warehouses, targeting specific business areas, became popular. Evolution of Data Warehousing•2000s: The Boom of Big Data •Emergence of Big Data: Data generation exploded, leading to the Big Data revolution. •Data Lakes: With the onset of Big Data, the concept of data lakes emerged to store raw, unprocessed data. •MPP (Massively Parallel Processing) Databases: Allowed for scalable query processing across distributed systems. •Cloud Adoption: Vendors like Amazon (with Redshift) and Google (with BigQuery) launched cloud-native data warehousing solutions. Evolution of Data Warehousing•2010s: Real-time and Advanced Analytics •Real-time Processing: Technologies like Apache Kafka allowed for real-time data ingestion. •Self-service BI: Tools like Tableau and Power BI democratized data access and analytics. •Snowflake: A new player, Snowflake, reimagined the cloud data warehouse, offering a multi-cloud solution. •Integration with AI/ML: Data warehouses began integrating with machine learning platforms for advanced analyticsEvolution of Data Warehousing•2020s: Modern and Hybrid Systems •The concept of the \"Lakehouse\" emerged, integrating the benefits of Data Lakes and Data Warehouses. There's an emphasis on real-time analytics, machine learning integration, and multi-cloud strategies. •The evolution of data warehousing has closely followed the broader trends in IT, from mainframe computing to cloud and now to serverless and multi-cloud architectures. It's a field that continually evolves in response to business needs and technological advancements. Key Roles of SQL, Python, R, Java, and Spark in Data Warehousing •SQL is foundational to data warehousing as it manages and queries relational databases that underpin these systems. •From loading, cleaning, transforming, querying to data security, SQL performs essential tasks in data warehousing. •Alongside SQL, several other languages contribute to data warehousing. Python, with its versatility, can craft custom data pipelines, process data, and even develop machine learning models.• R, known for its statistical prowess, is instrumental in data analysis and visual representation. •Java offers a vast platform for building diverse applications, including those tailored for data warehousing tasks like data loading and transformationKey Roles of SQL, Python, R, Java, and Spark in Data Warehousing •Spark's introduction has revolutionized modern data warehousing. •Its capacity to scale and handle enormous datasets swiftly positions it as an essential tool. •While SQL is lauded for its expressive nature and ease of use, especially for complex data tasks, Spark's strength lies in its ability to process intricate data types, such as JSON and Parquet. •Though Spark lacks native ACID (Atomicity, Consistency, Isolation, and Durability) transaction support, which SQL possesses, integrative approaches enable ACID transaction implementations in Spark-driven data warehouses. Key Roles of SQL, Python, R, Java, and Spark in Data Warehousing •The synergy between Spark and SQL is evident: companies use Spark for heavy-duty data tasks, and then SQL for detailed analysis. For instance, while a retailer may use Spark for data loading, SQL might be the go-to for data analysis like customer segmentation. •This symbiotic relationship between Spark and SQL is driving innovations in the evolving landscape of data warehousing. Key Roles of SQL, Python, R, Java, and Spark in Data Warehousing •Snowflake leverages Spark and SQL by: •Query pushdown: Snowflake allows Spark to offload complex processing work to Snowflake, significantly improving performance.•Snowpark: Snowpark is a Spark-based API that allows developers to run Spark code directly in Snowflake, eliminating the need to copy data between Snowflake and Spark.•Snowflake UDFs: Snowflake UDFs can be used in Spark SQL queries, giving developers access to the full power of Snowflake's data processing capabilities.Key Roles of SQL, Python, R, Java, and Spark in Data Warehousing •Databricks leverages Spark and SQL by: •Delta Lake: Delta Lake is a unified storage layer that combines the best features of data lakes and data warehouses, and is optimized for Spark.•Databricks SQL Analytics: Databricks SQL Analytics is a serverless SQL query engine that allows users to query Delta Live tables using standard SQL.•Databricks Spark Connector: The Databricks Spark Connector enables Spark to read and write data to Delta Lake tables.ETL-ProcessPurpose of ETL•ETL functions reshape the relevant data from the source systems into useful information to be stored in the data warehouse. Without these functions, there would be no strategic information in the data warehouse. If the source data is not extracted correctly, cleansed, and integrated in the proper formats, query processing, the backbone of the data warehouse, could not happen.Types of activities and tasks that compose the ETLprocess•Split one source data structure into several structures to go into several rows of the target database.•Read data from data dictionaries and catalogs of source systems.•Read data from a variety of file structures including flat files, indexed files (VSAM), and legacy system databases  (hierarchical/network).•Load details for populating atomic fact tables.•Aggregate for populating aggregate or summary fact tables.•Transform data from one format in the source platform to another format in the target platform.•Derive target values for input fields (example: age from date of birth).•Change cryptic values to values meaningful to the users (example: 1 and 2 to male and female).DATA EXTRACTIONDATA EXTRACTION•For operational systems upgrade, all you need is one-time extractions and data conversions.•For a data warehouse, you have to extract (increased complexity, 3rd party tools):•data from many disparate sources.•data on the changes for ongoing incremental loads as well as for a one-time initial full load.list of data extraction issues•Source Identification—identify source applications and source structures.•Method of extraction—for each data source, define whether the extraction process is manual or tool-based.•Extraction frequency—for each data source, establish how frequently the data extraction must by done—daily, weekly, quarterly, and so on.•Time window—for each data source, denote the time window for the extraction process.•Job sequencing—determine whether the beginning of one job in an extraction job stream has to wait until the previous job has finished successfully.•Exception handling—determine how to handle input records that cannot be extractedData in Operational Systems.Two cathegories:•Current Value. (most of the attributes) The value of an attribute remains constant only until a business transaction changes it. Data extraction for preserving the history of the changes in the data warehouse gets quite involved for this category of data.•Periodic Status. (not as common as the previous category) The history of the changes is preserved in the source systems themselves. Therefore, data extraction is relatively easier. Data in Operational Systems.•When you deploy your data warehouse, the initial data as of a certain time must be moved to the data warehouse to get it started. This is the initial load. •After the initial load, your data warehouse must be kept updated so the history of the changes and statuses are reflected in the data warehouse. There are two major types of data extractions from the source operational systems: •“as is” (static) data •data of revisions.Data in Operational Systems.•“As is” or static data is the capture of data at a given point in time. It is like taking a snapshot of the relevant source data at a certain point in time.•Data of revisions is also known as incremental data capture. Incremental data capture may be immediate or deferred. Within the group of immediate data capture there are three distinct options.Options for deferred data capture•Immediate Data Extraction.•Capture through Transaction Logs.• Capture through Database Triggers.•Capture in Source Applications.•Deferred Data Extraction.•Capture Based on Date and Time Stamp.•Capture by Comparing Files.Data capture through database triggers •Data capture through database triggers occurs right at the source and is therefore quite reliable.•You can capture both before and after images. •Building and maintaining trigger programs puts an additional burden on the development effort.•Execution of trigger procedures during transaction processing of the source systems puts additional overhead on the source systems.•This option is applicable only for source data in databases.Capture Based on Date and Time Stamp•Deletion of source records presents a special problem. If a source record gets deleted in between two extract runs, the information about the delete is not detected. •You can get around this by marking the source record for delete first, do the extraction run, and then go ahead and physically delete the record.• This means you have to add more logic to the source applications.Capture by Comparing Files•If none of the above techniques are feasible for specific source files in your environment, then consider this technique as the last resort. •This technique is also called the snapshot differential technique because it compares two snapshots of the source data.DATA TRANSFORMATIONDATA TRANSFORMATION•Extracted data is raw data and it cannot be applied to the data warehouse•All the extracted data must be made usable in the data warehouse.Quality of data•Major effort within data transformation is the improvement of data quality. •This includes filling in the missing values for attributes in the extracted data. •Data quality is of paramount importance (najvyššia dôležitosť) in the data warehouse because the effect of strategic decisions based on incorrect information can be devastating.Basic tasksin data transformation•Selection - beginning of the whole process of data transformation. Select either whole records or parts of several records from the source systems. •Splitting/joining - types of data manipulation needed to be performed on the selected parts of source records. Sometimes (uncommonly), you will be splitting the selected parts even further during data transformation. Joining of parts selected from many source systems is more widespread in the data warehouse environment.•Conversion - all-inclusive task. It includes a large variety of rudimentary (najzákladnejších) conversions of single fields for two primary reasons—one to standardize among the data extractions from disparate source systems, and the other to make the fields usable and understandable to the users.Basic tasksin data transformation(2)•Summarization. Sometimes it is not feasible to keep data at the lowest level of detail in the data warehouse. It may be that none of users ever need data at the lowest granularity for analysis or querying. •Enrichment - rearrangement and simplification of individual fields to make them more useful for the data warehouse environment. You may use one or more fields from the same input record to create a better view of the data for the data warehouse. This principle is extended when one or more fields originate from multiple records, resulting in a single field for the data warehouse.Major Transformation Types•Format Revisions•Decoding of Fields•Calculated and Derived Values.•Splitting of Single Fields.•Merging of Information.•Character Set Conversion.•Conversion of Units of Measurements•Date/Time Conversion. •Summarization. •Key Restructuring. •Deduplication. Data Integration and Consolidation•Entity Identification Problem•Multiple Sources ProblemEntity Identification Problem•If you have three different legacy applications developed in your organization at different times in the past, you are likely to have three different customer files supporting those systems. •Most of the customers will be common to all three files. •The same customer on each of the files may have a unique identification number. •These unique identification numbers for the same customer may not be the same across the three systems.•Solution - complex algorithms have to be designed to match records from all the three files and form groups of matching records. No matching algorithm can completely determine the groups. If the matching criteria are too tight, then some records will escape the groups. On the other hand, if the matching criteria are too loose, a particular group may include records of more than one customer.Multiple Sources Problem•Single data element having more than one source.•A straightforward solution is to assign a higher priority to one of the two sources and pick up the product unit cost from that source. Sometimes, a straightforward solution such as this may not sit well with needs of the data warehouse users. You may have to select from either of the files based on the last update date. Or, in some other instances, your determination of the appropriate source depends on other related fieldsDATA LOADINGDATA LOADING•Data loading takes the prepared data, applies it to the data warehouse, and stores it in the database•Terminology:•Initial Load — populating all the data warehouse tables for the very first time•Incremental Load — applying ongoing changes as necessary in a periodic manner•Full Refresh — completely erasing the contents of one or more tables and reloading with fresh data (initial load is a refresh of all the tables)Applying Data: Techniques and Processes•load, •append, •destructive•merge, •constructive merge.Load•If the target table to be loaded already exists and data exists in the table, the load process wipes out the existing data and applies the data from the incoming file. •If the table is already empty before loading, the load process simply applies the data from the incoming file.Append•extension of the load. •If data already exists in the table, the append process unconditionally adds the incoming data, preserving the existing data in the target table.• When an incoming record is a duplicate of an already existing record, you may define how to handle an incoming duplicate:• The incoming record may be allowed to be added as a duplicate. •In the other option, the incoming duplicate record may be rejected during the append process.Destructive Merge•Applies incoming data to the target data. •If the primary key of an incoming record matches with the key of an existing record, update the matching target record. •If the incoming record is a new record without a match with any existing record, add the incoming record to the target table.Constructive Merge•Slightly different from the destructive merge. •If the primary key of an incoming record matches with the key of an existing record, leave the existing record, add the incoming record, and mark the added record as superceding the old record.ETL Tools Options•Data transformation engines•Data capture through replication•Code generatorsData transformation engines•Consist of dynamic and sophisticated data manipulation algorithms. •The tool suite captures data from a designated set of source systems at user-defined intervals, performs elaborate data transformations, sends the results to a target environment, and applies the data to target files. •These tools provide maximum flexibility for pointing to various source systems, to select the appropriate data transformation methods, and to apply full loads and incremental loads. •The functionality of these tools sweeps the full range of the ETL process.Data capture through replication•Most of these tools use the transaction recovery logs maintained by the DBMS. •The changes to the source systems captured in the transaction logs are replicated in near real time to the data staging area for further processing. •Some of the tools provide the ability to replicate data through the use of database triggers. These specialized stored procedures in the database signal the replication agent to capture and transport the changes.Code generators•Tools that directly deal with the extraction, transformation, and loading of data. •The tools enable the process by generating program code to perform these functions. •Code generators create 3GL/4GL data extraction and transformation programs. •The tools generate most of the program code in some of the common programming languages. •Own program code can be addedd, also. •The code automatically generated by the tool has exits at which points you may add your code to handle special conditions.Big Data AnalyticsWeek 1 : Module 1: Introduction to Big Data and HadoopFaculty Name : Dr. Aditi Chhabria Dr. Smita Bharne\n",
      "Index -  Module :1 Introduction to Big Data and Hadoop\n",
      "252Lecture 1: Introduction to Big Data3Lecture 2: Hadoop Ecosystem 58\n",
      "Module :1 Introduction to Big Data and HadoopLecture 3: Introduction to spark3Lecture 4: Hadoop vs Spark58Introduction to Big DataLecture 1Introduction to Big Data \n",
      "Module :1 Introduction to Big Data and Hadoop254What is Big Data?How much data does it takes to be called Big Data?Where does big data come from?Where is the big data trend going?Who are some of the BIG DATA users?How will big data impact?Introduction to Big Data• In order to understand 'Big Data', we first need to know what'data'is????? DATA: \"The quantities, characters, or symbols on which operations are performed by a computer, which may be stored and transmitted in the form of electrical signals and recorded on magnetic, optical, or mechanical recording media.\"•  So, 'Big Data' is also adatabut with ahuge size. 'Big Data' is a term used to describe collection of data that is huge in size and yet growing exponentially with time. In short,such a data is so large and complex that none of the traditional data management tools are able to store it or process it efficiently.Module :1 Introduction to Big Data and Hadoop255What is Big Data?Introduction to Big Data\n",
      "Module :1 Introduction to Big Data and Hadoop256How much data does it takes to be called Big Data?\n",
      "How much data does it take to be called as Big Data????? Terrabytes to 10s of PetabytesWhat is not Big Data? A few gigabytesIntroduction to Big Data\n",
      "Module :1 Introduction to Big Data and Hadoop257§   Data come from many quarters.üSocial media sitesüSensorsüDigital photosüBusiness transactionsüLocation-based dataIntroduction to Big Data\n",
      "Module :1 Introduction to Big Data and Hadoop258Where does big data come from?Facts and Figures:üAnalysts predict that by 2020, there will be 5,200 gigabytes of data on every person in the world.üOn average, people send about 500 million tweet per day.üThe average U.S. customer uses 1.8 gigabytes of data per month on his or her cell phone planüWalmart processes one million customer transactions per hour.üAmazon sells 600 items per secondüMasterCard processes 74 billion transactions per year. üCommercial airlines make about 5,800 flights per day. Introduction to Big Data\n",
      "Module :1 Introduction to Big Data and Hadoop259TheNew York Stock Exchangegenerates aboutone terabyteof new trade data per day.Introduction to Big Data\n",
      "260Who are some of the BIG DATA users?\n",
      "Module :1 Introduction to Big Data and HadoopStatistic shows that500+terabytesof new data gets ingested into the databases of social media siteFacebook, every day. This data is mainly generated in terms of photo and video uploads, message exchanges, putting comments etc.Introduction to Big Data\n",
      "Module :1 Introduction to Big Data and Hadoop261Who are some of the BIG DATA users?\n",
      "SingleJet enginecan generate10+terabytesof data in30 minutesof a flight time. With many thousand flights per day, generation of data reaches up to manyPetabytes..Introduction to Big Data\n",
      "Module :1 Introduction to Big Data and Hadoop262Who are some of the BIG DATA users?\n",
      "Why is Big Data Important?The importance of big data doesn’t revolve around how much data you have, but what you do with it. You can take data from any sources and analyze it to find answers that enable1.Cost reductions2.Time reductions3.New product development and optimized offerings, and4.Smart decision making Introduction to Big Data\n",
      "Module :1 Introduction to Big Data and Hadoop263How will big data impact?Big Data Characteristics and Types of Big DataBig data' could be found in three forms:üStructuredüUnstructuredüSemi-structuredCategories of Big Data\n",
      "Module :1 Introduction to Big Data and Hadoop265Structured DataüAny data that can be stored, accessed and processed in the form of fixed format is termed as a 'structured' data. üOver the period of time, talent in computer science have achieved greater success in developing techniques for working with such kind of data (where the format is well known in advance) and also deriving value out of it. üHowever, now days, we are foreseeing issues when size of such data grows to a huge extent, typical sizes are being in the rage of multiple zettabyte.Categories of Big Data\n",
      "Module :1 Introduction to Big Data and Hadoop266Example of Structured DataAn 'Employee' table in a database is an example of Structured DataCategories of Big Data\n",
      "Module :1 Introduction to Big Data and Hadoop267\n",
      "UnstructuredüAny data with unknown form or the structure is classified as unstructured data. In addition to the size being huge, un-structured data poses multiple challenges in terms of its processing for deriving value out of it. üTypical example of unstructured data is, a heterogeneous data source containing a combination of simple text files, images, videos etc.ü Now a day organizations have wealth of data available with them but unfortunately they don't know how to derive value out of it since this data is in its raw form or unstructured format.Categories of Big Data\n",
      "Module :1 Introduction to Big Data and Hadoop268Example of Unstructured DataOutput returned by 'Google Search'Categories of Big Data\n",
      "Module :1 Introduction to Big Data and Hadoop269\n",
      "Semi-structuredüSemi-structured data can contain both the forms of data. üWe can see semi-structured data as a structured in form but it is actually not defined with e.g. a table definition in relational DBMS. üExample of semi-structured data is a data represented in XML file.Categories of Big Data\n",
      "Module :1 Introduction to Big Data and Hadoop270Example of Semi-structured DataPersonal data stored in a XML file-Categories of Big Data\n",
      "Module :1 Introduction to Big Data and Hadoop271\n",
      "Please note that web application data, which is unstructured, consists of log files, transaction history files etc. OLTP systems are built to work with structured data wherein data is stored in relations (tables).Data Growth over years\n",
      "Module :1 Introduction to Big Data and Hadoop272\n",
      "Characterization of Big Data: Volume, Velocity, Variety, Veracity and Value\n",
      "Module :1 Introduction to Big Data and Hadoop273Volume: •The name 'Big Data' itself is related to a size which is enormous. •Size of data plays very crucial role in determining value out of data. Also, whether a particular data can actually be considered as a Big Data or not, is dependent upon volume of data. •Hence,'Volume'is one characteristic which needs to be considered while dealing with 'Big Data'.\n",
      "Characterization of Big Data: Volume, Velocity, Variety, Veracity and Value\n",
      "Module :1 Introduction to Big Data and Hadoop274Velocity•The term'velocity'refers to the speed of generation of data. How fast the data is generated and processed to meet the demands, determines real potential in the data.•It deals with the speed at which data flows in from sources like business processes, application logs, networks and social media sites, sensors,Mobile devices, etc. The flow of data is massive and continuous.\n",
      "Characterization of Big Data: Volume, Velocity, Variety, Veracity and Value\n",
      "Module :1 Introduction to Big Data and Hadoop275Variety•Variety refers to heterogeneous sources and the nature of data, both structured and unstructured. •During earlier days, spreadsheets and databases were the only sources of data considered by most of the applications. •Now days, data in the form of emails, photos, videos, PDFs, audio, etc. is also being considered in the analysis applications. This variety of unstructured data poses certain issues for storage, mining and analysing data.\n",
      "Characterization of Big Data: Volume, Velocity, Variety, Veracity and Value\n",
      "Module :1 Introduction to Big Data and Hadoop276Veracity: Trustworthiness of Data•Data involves some uncertainty and ambiguities•Mistakes can be introduced by humans and machinesüPeople sharing accountsüLike it today, dislike it tomorrorwüWrong system timestamps•Data Quality is vital!•Analytics and conclusions rely on good data qualityGarbage data + perfect model =>   garbage results       Perfect data + garbage model => garbage results.\n",
      "Characterization of Big Data: Volume, Velocity, Variety, Veracity and Value\n",
      "Module :1 Introduction to Big Data and Hadoop277Value•Raw data of Big Data is of low value         For example, single observations•Analytics and theory about the data increases the value•Analytics transform big data into smart data!\n",
      "§A typical PC might have had 10 gigabytes of storage in 2000. §Today, Facebook ingests 500 terabytes of new data every day.§Boeing 737 will generate 240 terabytes of flight data during a single flight across the US.§ The smart phones, the data they create and consume; sensors embedded into everyday objects will soon result in billions of new, constantly-updated data feeds containing environmental, location, and other information, including video. Big Data characteristics(1st Character of Big Data Volume)\n",
      "Module :1 Introduction to Big Data and Hadoop278§Clickstreams and ad impressions capture user behavior at millions of events per second§ High-frequency stock trading algorithms reflect market changes within microseconds§ Machine to Machine processes exchange data between billions of devices§ Infrastructure and sensors generate massive log data in real-time§ On-line gaming systemssupport millions of concurrent users, each producing multiple inputs per second.Big Data characteristics(2nd Character of Big Data Velocity)\n",
      "Module :1 Introduction to Big Data and Hadoop279§Big Data isn't just numbers, dates, and strings. Big Data is also geospatial data, 3D data, audio and video, and unstructured text, including log files and social media.§Traditional database systems were designed to address smaller volumes of structured data, fewer updates or a predictable, consistent data structure.§Big Data analysis includes different types of data Big Data characteristics(3rd Character of Big Data Variety)\n",
      "Module :1 Introduction to Big Data and Hadoop280§Relational Data (Tables/Transaction/Legacy Data)§Text Data (Web)§Semi-structured Data (XML) §Graph Data§Social Network, Semantic Web (RDF), … §Streaming Data §You can only scan the data once§A single application can be generating/collecting many types of data  §Big Public Data (online, weather, finance, etc)  Big Data characteristics(3rd Character of Big Data Variety)\n",
      "Module :1 Introduction to Big Data and Hadoop281Lecture 2  What is Hadoop?What is Hadoop??\n",
      "Module :1 Introduction to Big Data and Hadoop283\n",
      "§Open source framework  of tools designed for storage and processing of large scale data (BigData)§Hadoop is maintained by Apache§Created by Doug Cutting and Mike Carafella in 2005.§Cutting named the program after his son’s toy elephant.What is Hadoop?\n",
      "Module :1 Introduction to Big Data and Hadoop284HadoopFramework of ToolsisHadoop’s\tDevelopers\n",
      "Doug Cutting2005: Doug Cutting and Michael J. Cafarella developed Hadoop to support distribution for theNutchsearch engine project.The project was funded by Yahoo.2006: Yahoo gave the project to Apache Software Foundation.\n",
      "Module :1 Introduction to Big Data and Hadoop285285Do we remember Big Data Challenge Points?????\n",
      "Module :1 Introduction to Big Data and Hadoop286VelocityVolumeVarietyTraditional Approach\n",
      "Module :1 Introduction to Big Data and Hadoop287Big DataPowerful ComputerProcessed BYBig DataProcessing LimitOnly so much data could be processed Hadoop Approach\n",
      "Module :1 Introduction to Big Data and Hadoop288Big Databroken into piecesHadoop Approach\n",
      "Module :1 Introduction to Big Data and Hadoop289ComputationComputationComputationComputationCombined ResultsMove computation to the dataBig DataHadoop Assumptions\n",
      "Module :1 Introduction to Big Data and Hadoop290Hadoop  was developed with large clusters of computers in mind with the following assumptionsüHardware will fail, Since it considers a large cluster of computers.üProcessing will be run in batches, so aim at high throughput as opposed to low latency.üApplications that run on Hadoop Distributed File System(HDFS) have large data sets typically from gigabyte to terabytes in size.süPortability is important.üAvailability of high-aggregate data bandwidth and scale to hundreds of nodes in a single cluster.üShould support tens of millions of files in a single instance.üApplications need a write-once-read-many access model.Hadoop Components; Hadoop EcosystemHadoop Components\n",
      "Module :1 Introduction to Big Data and Hadoop292\n",
      "• Hadoop has two main components: MapReduce and HDFS File system• Hadoop has set of tools and those set of tools are represented in Hadoop Ecosystem•HDFS is a file system written in Java•Responsible for storing data on the cluster•Data files are split into blocks and distributed across the nodes in the cluster•Each block is replicated multiple timesHadoop Distributed File System(HDFS)\n",
      "Module :1 Introduction to Big Data and Hadoop293\n",
      "HDFS Architecture\n",
      "Module :1 Introduction to Big Data and Hadoop294\n",
      "§Main Components of HDFS-NameNode : keeps track of which blocks make up a file and where they are stored-DataNodes: slaves which provide the actual storage and are deployed on each machine. HDFS Architecture\n",
      "Module :1 Introduction to Big Data and Hadoop295\n",
      "•Massive Parallel Processing Technique for processing data which is distributed on a commodity cluster.•How this is achieved?üA method for distributing computation across multiple nodesüEasy to ParallelizeüEach node processes the data that is stored at that nodeüConsists of two main phases  Map  ReduceMapReduce\n",
      "Module :1 Introduction to Big Data and Hadoop296\n",
      "•Reads data as key/value pairs•Outputs zero or more key/value pairsThe Mapper\n",
      "Module :1 Introduction to Big Data and Hadoop297\n",
      "Shuffling\n",
      "Module :1 Introduction to Big Data and Hadoop298The intermediate result is sorted and redistributed.\n",
      "Shuffled and sorted data is processed as per key in parallel.Reducing\n",
      "Module :1 Introduction to Big Data and Hadoop299Overall MapReduce word count process\n",
      "Module :1 Introduction to Big Data and Hadoop300\n",
      "•Hadoop works on distributed model.•All these are set of  low cost computers.•Hadoop works on Linux based machinesHadoop Architecture\n",
      "Module :1 Introduction to Big Data and Hadoop301\n",
      "•Each machine consists of two components: Task Tracker and Data Node•Task Tracker job is to process smaller piece of task given to this particular node and Data node component is to manage data that is received on this particular node.Hadoop Architecture\n",
      "Module :1 Introduction to Big Data and Hadoop302\n",
      "SlavesHadoop Architecture\n",
      "Module :1 Introduction to Big Data and Hadoop303\n",
      "MasterSlaveHadoop Architecture\n",
      "Module :1 Introduction to Big Data and Hadoop304\n",
      "MapReduceHDFS\n",
      "Hadoop Architecture\n",
      "Module :1 Introduction to Big Data and Hadoop305\n",
      "ApplicationQueueJob Tracker Role of job tracker is to break the higher bigger task into smaller pieces and forward it to the task tracker. Task tracker in turn will perform the computations and results are forwarded back to the job tracker. The final result is combined by job tracker.Name NodeName node running on the master node is responsible to keep an index of which data resides on which data node. So name node tells application to go to this particular node where data is residing. So application don’t have to depend on Name node.Hadoop Architecture\n",
      "Module :1 Introduction to Big Data and Hadoop306Hadoop Features : Fault Tolerance\n",
      "Module :1 Introduction to Big Data and Hadoop307\n",
      "Fault Tolerance for data•Hardware failures are bound to happen. They will happen. •Hadoop has built-in hardware failure management.•Hadoop maintains  3 copies of each file, so this way failure management is carried out with data replication.Hadoop Features : Data Replication\n",
      "Module :1 Introduction to Big Data and Hadoop308Default Replication is 3-fold\n",
      "Hadoop Features : Scalable\n",
      "Module :1 Introduction to Big Data and Hadoop309\n",
      "Scalable•Hadoop is higly scalable. •It is designed to scale up from single servers to thousands of machines, each offering local computation and storageScalable1000sEasy Programming\n",
      "Module :1 Introduction to Big Data and Hadoop310\n",
      "Programmers  could focus on writing scale free programsHadoop Ecosystem\n",
      "Module :1 Introduction to Big Data and Hadoop311\n",
      "Hadoop Ecosystem (Flume)\n",
      "Module :1 Introduction to Big Data and Hadoop312\n",
      "Hadoop Ecosystem (Hive)\n",
      "Module :1 Introduction to Big Data and Hadoop313\n",
      "Hadoop Ecosystem (HBase)\n",
      "Module :1 Introduction to Big Data and Hadoop314\n",
      "Hadoop Ecosystem (Mahout)\n",
      "Module :1 Introduction to Big Data and Hadoop315\n",
      "Hadoop Ecosystem (Pig)\n",
      "Module :1 Introduction to Big Data and Hadoop316\n",
      "Hadoop Ecosystem (Sqoop)\n",
      "Module :1 Introduction to Big Data and Hadoop317\n",
      "•ScalableüBreaks data into smaller equal pieces (blocks, typically64/128 Mb)üBreaks big computation task down into smaller individual tasksüMore slaves, more processing and storage power•Cheapü  Commodity hardware, open source software•Extremely fault tolerant•“Easy” to useWhy Hadoop??????\n",
      "Module :1 Introduction to Big Data and Hadoop318Lecture 3:Introduction to spark§To answer this, we have to look at the concept of batch and real-time processing. Hadoop is based on the concept of batch processing where the processing happens of blocks of data that have already been stored over a period of time. §At the time, Hadoop broke all the expectations with the revolutionary MapReduce framework in 2005. Hadoop MapReduce is the best framework for processing data in batches.§This went on until 2014, till Spark overtook Hadoop. The USP for Spark was that it could process data in real time and was about 100 times faster than Hadoop MapReduce in batch processing large data sets.Why Spark when Hadoop is already there?\n",
      "Module :1 Introduction to Big Data and Hadoop320Apache Spark is an open-source cluster computing framework for real-time processing. It has a thriving open-source community and is the most active Apache project at the moment. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance. What is Apache Spark?\n",
      "Module :1 Introduction to Big Data and Hadoop321\n",
      "Apache Spark is an open source cluster computing framework for real-time data processing. The main feature of Apache Spark is its in-memory cluster computing that increases the processing speed of an application. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. It is designed to cover a wide range of workloads such as batch applications, iterative algorithms, interactive queries, and streaming.Spark & its Features\n",
      "Module :1 Introduction to Big Data and Hadoop322\n",
      "1.SpeedSpark runs up to 100 times faster than Hadoop MapReduce for large-scale data processing. It is also able to achieve this speed through controlled partitioning.2.Powerful CachingSimple programming layer provides powerful caching and disk persistence capabilities.3.DeploymentIt can be deployed throughMesos, Hadoop via YARN, or Spark’s own cluster manager.4.Real-TimeIt offers Real-time computation & low latency because ofin-memory computation.5.PolyglotSparkprovides high-level APIs in Java, Scala, Python, and R. Spark code can be written in any of these four languages. It also provides a shell in Scala and Python.Features of Apache Spark:\n",
      "Module :1 Introduction to Big Data and Hadoop323Apache Spark has a well-defined layered architecture where all the spark components and layers are loosely coupled. This architecture is further integrated with various extensions and libraries. Apache Spark Architecture is based on two main abstractions:•Resilient Distributed Dataset (RDD)•Directed Acyclic Graph (DAG)Spark Architecture Overview\n",
      "Module :1 Introduction to Big Data and Hadoop324\n",
      "Fig: Spark ArchitectureSpark ecosystem is composed of various components like Spark SQL, Spark Streaming, MLlib, GraphX, and the Core API component.Spark Eco-System\n",
      "Module :1 Introduction to Big Data and Hadoop325\n",
      "•Spark Core•Spark Core is the base engine for large-scale parallel and distributed data processing. Further, additional libraries which are built on the top of the core allows diverse workloads for streaming, SQL, and machine learning. It is responsible for memory management and fault recovery, scheduling, distributing and monitoring jobs on a cluster & interacting with storage systems.•Spark Streaming•Spark Streaming is the component of Spark which is used to process real-time streaming data. Thus, it is a useful addition to the core Spark API. It enables high-throughput and fault-tolerant stream processing of live data streams.•Spark SQL•Spark SQL is a new module in Spark which integrates relational processing with Spark’s functional programming API. It supports querying data either via SQL or via the Hive Query Language. For those of you familiar with RDBMS, Spark SQL will be an easy transition from your earlier tools where you can extend the boundaries of traditional relational data processing.Spark Eco-System\n",
      "Module :1 Introduction to Big Data and Hadoop326•GraphX•GraphX is the Spark API for graphs and graph-parallel computation. Thus, it extends the Spark RDD with a Resilient Distributed Property Graph. At a high-level, GraphX extends the Spark RDD abstraction by introducing the Resilient Distributed Property Graph (a directed multigraph with properties attached to each vertex and edge).\\•MLlib (Machine Learning)•MLlib stands for Machine Learning Library. Spark MLlib is used to perform machine learning in Apache Spark.•SparkR•It is an R package that provides a distributed data frame implementation. It also supports operations like selection, filtering, aggregation but on large data-sets.Spark Eco-System\n",
      "Module :1 Introduction to Big Data and Hadoop327Resilient Distributed Dataset(RDD)RDDs are the building blocks of any Spark application. RDDs Stands for:•Resilient: Fault tolerant and is capable of rebuilding data on failure•Distributed: Distributed data among the multiple nodes in a cluster•Dataset: Collection of partitioned data with valueFundamental Data Structure of Spark, i.e. RDD.\n",
      "Module :1 Introduction to Big Data and Hadoop328\n",
      "•It is a layer of abstracted data over the distributed collection. It is immutable in nature and follows lazy transformations. •Data in an RDD is split into chunks based on a key. RDDs are highly resilient, i.e, they are able to recover quickly from any issues as the same data chunks are replicated across multiple executor nodes. •Thus, even if one executor node fails, another will still process the data. This allows you to perform your functional calculations against your dataset very quickly by harnessing the power of multiple nodesRDD\n",
      "Module :1 Introduction to Big Data and Hadoop329In your master node, you have the driver program, which drives your application. The code you are writing behaves as a driver program or if you are using the interactive shell, the shell acts as the driver program.Working of Spark Architecture\n",
      "Module :1 Introduction to Big Data and Hadoop330\n",
      "•Inside the driver program, the first thing you do is, you create a Spark Context. Assume that the Spark context is a gateway to all the Spark functionalities. It is similar to your database connection. Any command you execute in your database goes through the database connection. Likewise, anything you do on Spark goes through Spark context.•Now, this Spark context works with the cluster manager to manage various jobs. The driver program & Spark context takes care of the job execution within the cluster. A job is split into multiple tasks which are distributed over the worker node. •Worker nodes are the slave nodes whose job is to basically execute the tasks. These tasks are then executed on the partitioned RDDs in the worker node and hence returns back the result to the Spark Context.Working of Spark Architecture\n",
      "Module :1 Introduction to Big Data and Hadoop331•Spark Context takes the job, breaks the job in tasks and distribute them to the worker nodes. These tasks work on the partitioned RDD, perform operations, collect the results and return to the main Spark Context.•If you increase the number of workers, then you can divide jobs into more partitions and execute them parallelly over multiple systems. It will be a lot faster.•With the increase in the number of workers, memory size will also increase & you can cache the jobs to execute it faster.\n",
      "Module :1 Introduction to Big Data and Hadoop332Workflow of Spark Architecture with Real World Scenario\n",
      "Module :1 Introduction to Big Data and Hadoop333\n",
      "STEP 1: The client submits spark user application code. When an application code is submitted, the driver implicitly converts user code that contains transformations and actions into a logically directed acyclic graph called DAG. At this stage, it also performs optimizations such as pipelining transformations.STEP 2: After that, it converts the logical graph called DAG into physical execution plan with many stages. After converting into a physical execution plan, it creates physical execution units called tasks under each stage. Then the tasks are bundled and sent to the cluster.\n",
      "Module :1 Introduction to Big Data and Hadoop334Workflow of Spark Architecture with Real World ScenarioSTEP 3: Now the driver talks to the cluster manager and negotiates the resources. Cluster manager launches executors in worker nodes on behalf of the driver. At this point, the driver will send the tasks to the executors based on data placement. When executors start, they register themselves with drivers. So, the driver will have a complete view of executors that are executing the task.\n",
      "Module :1 Introduction to Big Data and Hadoop335Workflow of Spark Architecture with Real World Scenario\n",
      "STEP 4:During the course of execution of tasks, driver program will monitor the set of executors that runs.Driver node also schedules future tasks based on data placement.Lecture 4:Hadoop vs Spark•Hadoop is based on the concept of batch processing where the processing happens of blocks of data that have already been stored over a period of time.•At the time, Hadoop broke all the expectations with the revolutionary MapReduce framework in 2005. Hadoop MapReduce is the best framework for processing data in batches.•The USP for Spark was that it couldprocess datainreal timeand was about 100 times faster than Hadoop MapReducein batch processing large data sets.Hadoop vs Spark--Why Spark when we have Hadoop already?.\n",
      "Module :1 Introduction to Big Data and Hadoop337Hadoop vs Spark-Differences between Hadoop and Spark\n",
      "Module :1 Introduction to Big Data and Hadoop338\n",
      "Hadoop vs Apache Spark\n",
      "Module :1 Introduction to Big Data and Hadoop339FeaturesHadoopApache SparkData ProcessingApache Hadoop provides batch processingApache Spark provides both batch processing and stream processingMemory usageHadoop is disk-boundSpark uses large amounts of RAMSecurityBetter security featuresIts security is currently in its infancylt ToleranceReplication is used for fault tolerance.RDD and various data storage models are used for fault tolerance.Graph ProcessingAlgorithms like PageRank is used.Spark comes with a graph computation library called GraphX.Ease of UseDifficult to use.Easier to use.Real-time data processingIt fails when it comes to real-time data processing.It can process real-time data.SpeedHadoop’s MapReduce model reads and writes from a disk, thus it slows down the processing speed.Spark reduces the number of read/write cycles to disk and store intermediate data in memory, hence faster-processing speed.LatencyIt is high latency computing framework.It is a low latency computing and can process data interactivelyThank You\n"
     ]
    }
   ],
   "source": [
    "# A very basic function to extract text from pdf\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    extracted_text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        for page_number in range(num_pages):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            extracted_text += page.extract_text()\n",
    "    print(extracted_text)\n",
    "    return extracted_text\n",
    "\n",
    "pdf_path = \" pdf address \"  # Don't forget to give your address here \n",
    "text = extract_text(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = text.replace(\"Module 1– Introduction to Social Network\",\" \")\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most important step of chunking your textual data. Here you could use complete creativity while fine-tuning\n",
    "# My suggestion try performing content-based chunking\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,    #for next time let's try to keep the chunk size little bigger\n",
    "    chunk_overlap = 100,\n",
    "    length_function = len,\n",
    "    add_start_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50create table course (    course_id char(5) primary key,    title             varchar(20), dept_name varchar(20) references department)create table course (    …    dept_name varchar(20),    foreign key (dept_name) references department                on delete cascade                on update cascade,    . . . )alternative actions to cascade:  set null, set defaultThe CHECK Constraint§Defines a condition that each row must satisfy..., salary NUMBER(2)     CONSTRAINT emp_salary_min              CHECK (salary > 0),...Adding a Constraint Syntax§Use the ALTER TABLE statement to:•Add or drop a constraint, but not modify its structure•Enable or disable constraints•Add a NOT NULL constraint by using the MODIFY clauseALTER TABLE  table  ADD [CONSTRAINT constraint] type (column);Adding a ConstraintAdd a FOREIGN KEY constraint to the EMPLOYEES table indicating that a manager must already exist as a valid employee in the EMPLOYEES table.ALTER TABLE     employeesADD CONSTRAINT  emp_manager_fk\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([text])\n",
    "print(texts[14].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data Management & Visualization (DMV) Module 1Structured Query Language (SQL)Index\\n2Lecture  1Overview of SQL4Lecture  1Data Types, Data Definition Commands. 16Lecture  2Data Manipulation commands32Lecture  2Integrity constraints ; Referential integrity , check constraints 38Lecture 1 Overview of SQLOverview of SQL- What is SQL?§Standard command set used to communicate with relational DBMS.§ All tasks related to Relational Database Management like creating table, querying the database for information , modifying data in the database.Advantages§High level  language that provides a greater degree of abstraction than procedural languages.§Allow the end-users to deal with number of database management systems where it is available.§Simple and easy to learn can handle complex situation.§All SQL operations are performed at set level.§For digital systems, the variable takes on discrete values.4Data Models', '5Model ofsystemin client’smindEntity model ofclient’s modelTable modelof entity modelTables on diskOracleserverDefinition of a Relational Database\\nRelational Model and relational Algebra6A relational database is a collection of relations or two-dimensional tables.\\nOracle\\nserverTable Name: EMPLOYEESTable Name: DEPARTMENTS……\\n7Relational Database Management System\\nUser tablesData dictionaryServer8Communicating with a RDBMS Using SQL\\nSELECT department_name FROM   departments;SQL statementis entered.OracleserverStatement is sent to Oracle Server.\\nAdvantages of DBMS§No redundant data §Data Consistency§Secure§Privacy – Limited access§Easy access to data§Easy recovery\\n9Relational Model \\n10NameAddressPhone_noDOBSemester Smith...IIIJones...IIICurry...IIIJack...IIIStudentRelation\\nRecord/tupleColumn/Attribute\\nDomain Example 2-Employee Relation\\n11IDNameAgeSalary1Adam34130002Alex28150003Stuart20180004Ross4219020What is a Record/Tuple, Column/Attribute, Domain?']\n"
     ]
    }
   ],
   "source": [
    "# Combinig chunks of your textual data to form a List. \n",
    "\n",
    "docs = []\n",
    "\n",
    "for chunk in texts:\n",
    "    docs.append(chunk.page_content)\n",
    "\n",
    "print(docs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        model = 'models/embedding-001'\n",
    "        title = 'LinkedIn API'\n",
    "        return genai.embed_content(\n",
    "            model=model,\n",
    "            content=input,\n",
    "            task_type=\"retrieval_document\",\n",
    "            title=title)[\"embedding\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define and create you Vector Databse of chromadb in Database folders\n",
    "\n",
    "def create_chroma_db(docs,name):\n",
    "    chroma_client = chromadb.PersistentClient(path=\"db address\") #Don't forget to change path\n",
    "    db = chroma_client.get_or_create_collection(\n",
    "        name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "    \n",
    "    initial_size = db.count()\n",
    "    for i, d in tqdm(enumerate(docs), total=len(docs), desc=\"Creating Chroma DB\"):\n",
    "        db.add(\n",
    "            documents=d,\n",
    "            ids=str(i + initial_size)\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "    return db\n",
    "\n",
    "\n",
    "def get_chroma_db(name):\n",
    "    chroma_client = chromadb.PersistentClient(path=\"db address 2\") # Here as well \n",
    "    return chroma_client.get_collection(name=name, function=EmbeddingFunction())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Chroma DB: 100%|██████████| 131/131 [02:15<00:00,  1.04s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11228"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating your Vector Database. [Don't unneccessarily run this code]\n",
    "\n",
    "db = create_chroma_db(docs, \"sme_db\")\n",
    "db.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>metadatas</th>\n",
       "      <th>documents</th>\n",
       "      <th>uris</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.015172463841736317, -0.010413325391709805, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Python Tutorial\\nRelease 3.7.0\\nGuido van Ross...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.03553083539009094, 0.01233371626585722, -0....</td>\n",
       "      <td>None</td>\n",
       "      <td>4.3 The range() Function . . . . . . . . . . ....</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[0.019464796409010887, -0.03204401955008507, -...</td>\n",
       "      <td>None</td>\n",
       "      <td>and tools, and additional documentation.\\nThe ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>[-0.0029656183905899525, 0.003489450318738818,...</td>\n",
       "      <td>None</td>\n",
       "      <td>Yuck!\\nC&gt;\\nThese two variables are only deﬁned...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>[-0.03242751583456993, -0.009994188323616982, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>compare unequal (except with themselves), and ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11092</th>\n",
       "      <td>9877</td>\n",
       "      <td>[-0.009271173737943172, -0.08612344413995743, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>UNDER_DELIVER Y_AUDIENCE_EXP ANSION\\nOVER_DELI...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11093</th>\n",
       "      <td>9878</td>\n",
       "      <td>[0.02701648883521557, -0.07625408470630646, -0...</td>\n",
       "      <td>None</td>\n",
       "      <td>urn:li:sponsoredCampaign:12244313 and urn:li:s...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11094</th>\n",
       "      <td>9879</td>\n",
       "      <td>[0.03511833772063255, -0.06084388121962547, -0...</td>\n",
       "      <td>None</td>\n",
       "      <td>HTTP\\nField\\nNameRequir ed Descr iption\\nentit...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11095</th>\n",
       "      <td>988</td>\n",
       "      <td>[0.012748446315526962, 0.019533473998308182, -...</td>\n",
       "      <td>None</td>\n",
       "      <td>Zero-dimensional buﬀers are C and Fortran cont...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11096</th>\n",
       "      <td>9880</td>\n",
       "      <td>[0.008453541435301304, -0.05648592859506607, -...</td>\n",
       "      <td>None</td>\n",
       "      <td>SponsoredCampaign and SponsoredCampaignGroup)\\...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11097 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ids                                         embeddings metadatas  \\\n",
       "0         0  [0.015172463841736317, -0.010413325391709805, ...      None   \n",
       "1         1  [0.03553083539009094, 0.01233371626585722, -0....      None   \n",
       "2        10  [0.019464796409010887, -0.03204401955008507, -...      None   \n",
       "3       100  [-0.0029656183905899525, 0.003489450318738818,...      None   \n",
       "4      1000  [-0.03242751583456993, -0.009994188323616982, ...      None   \n",
       "...     ...                                                ...       ...   \n",
       "11092  9877  [-0.009271173737943172, -0.08612344413995743, ...      None   \n",
       "11093  9878  [0.02701648883521557, -0.07625408470630646, -0...      None   \n",
       "11094  9879  [0.03511833772063255, -0.06084388121962547, -0...      None   \n",
       "11095   988  [0.012748446315526962, 0.019533473998308182, -...      None   \n",
       "11096  9880  [0.008453541435301304, -0.05648592859506607, -...      None   \n",
       "\n",
       "                                               documents  uris  data  \n",
       "0      Python Tutorial\\nRelease 3.7.0\\nGuido van Ross...  None  None  \n",
       "1      4.3 The range() Function . . . . . . . . . . ....  None  None  \n",
       "2      and tools, and additional documentation.\\nThe ...  None  None  \n",
       "3      Yuck!\\nC>\\nThese two variables are only deﬁned...  None  None  \n",
       "4      compare unequal (except with themselves), and ...  None  None  \n",
       "...                                                  ...   ...   ...  \n",
       "11092  UNDER_DELIVER Y_AUDIENCE_EXP ANSION\\nOVER_DELI...  None  None  \n",
       "11093  urn:li:sponsoredCampaign:12244313 and urn:li:s...  None  None  \n",
       "11094  HTTP\\nField\\nNameRequir ed Descr iption\\nentit...  None  None  \n",
       "11095  Zero-dimensional buﬀers are C and Fortran cont...  None  None  \n",
       "11096  SponsoredCampaign and SponsoredCampaignGroup)\\...  None  None  \n",
       "\n",
       "[11097 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(db.peek(11097))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.DataFrame(db.peek(10)).iloc[0][\"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve fact from the vector database\n",
    "\n",
    "def get_relevant_passages(query, db, n_results):\n",
    "    passages = db.query(query_texts=[query], n_results=n_results)['documents'][0]\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_response(response):\n",
    "    # Initialize an empty string to accumulate text\n",
    "    extracted_text = \"\"\n",
    "    \n",
    "    # Check if the response has a 'parts' attribute and iterate over it if present\n",
    "    if hasattr(response, 'parts'):\n",
    "        for part in response.parts:\n",
    "            if hasattr(part, 'text'):  # Ensure the part has a 'text' attribute\n",
    "                extracted_text += part.text + \"\\n\"  # Append the text from each part\n",
    "    \n",
    "    # Alternatively, if the structure is deeper (e.g., candidates with content parts):\n",
    "    elif hasattr(response, 'candidates') and len(response.candidates) > 0:\n",
    "        for candidate in response.candidates:\n",
    "            for part in candidate.content.parts:\n",
    "                if hasattr(part, 'text'):\n",
    "                    extracted_text += part.text + \"\\n\"\n",
    "    \n",
    "    return extracted_text.strip()  # Return the combined text, stripping any trailing newline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(ques, knowledge, chats):\n",
    "    text = knowledge.replace(\"'\",\"\").replace('\"','') #even i dont know why i did this\n",
    "\n",
    "\n",
    "    # Choose from the given prompts which suits you the BEST\n",
    "    # If you want you may create your own prompt by referring to the following options\n",
    "    # --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # prompt = f\"\"\"question: {ques}.\\n\n",
    "    # information base or knowledge base: {text}\\n\n",
    "    # Answer the question in 1250-2500 words strictly based on the information base or knowledge base given, \\n\n",
    "    # If the question is asking for a code then also explain the algorithm, if the question is asking for career guidance then provide complete career guidance and links for various courses, if difference is asked differentiate with minimum 7 points in tabular format and one example\n",
    "    # if the information is not sufficient then give output as \"Info not good to answer\"\n",
    "    # \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # prompt = f\"\"\"question: {ques}.\\n\n",
    "    # information base or knowledge base: {text}\\n\n",
    "    # Answer the question strictly based from {text} by filtering the required information from {text}\\n\n",
    "    # Generate a sophisticated and neat answer such that it could be written in exam\\n\n",
    "    # if the information is not sufficient then use keywords from {text} related to {ques} and answer the queestion and at the end of answer mention \"[from other sources as well]\" BUT try to give answers from only and only{text}\\n\n",
    "    # If the question is asking for a code then also explain the algorithm,\\n \n",
    "    # if the question is asking for career guidance then provide complete career guidance and links for various courses, \\n\n",
    "    # if difference is asked differentiate with minimum 7 points in tabular format and one example\n",
    "    # MOST IMPORTANTLY: answer the questions from {text} in such a way to write in exam\\n\n",
    "    # NOTE: try to answwer {ques} from {text} as much as possible\n",
    "    # \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # prompt = f\"\"\"question: {ques}.\\n\n",
    "    # chat histoty: {chats}\\n\n",
    "    # information base or knowledge base: {text}\\n\n",
    "    # Work like a answer generarting bot for exam who generates answer for given {ques} based on the {text} provided. Generate output in such a way that it should be written in examination.\n",
    "    # check whether {ques} is a question or an instruction, if its an instruction give an affirmative response \\n\n",
    "    # ELSE\n",
    "    # Answer the question STRICTLY based from {text} by filtering the required information from {text}\\n\n",
    "    # if the information is not sufficient then use keywords from {text} related to {ques} and answer the queestion and at the end of answer mention \"[from other sources as well]\" BUT try to give answers from only and only{text}\\n\n",
    "    # If the question is asking for a code then also explain the algorithm,\\n \n",
    "    # if the question is asking for career guidance then provide complete career guidance and links for various courses, \\n\n",
    "    # if difference is asked differentiate with minimum 7 points in tabular format and one example\\n\n",
    "    # Also DO NOT forget to consider the previous qquestions and the answers you gave for it :{chats}\\n\n",
    "    # If the question refers about past questions or answers, generate the answer considering {chats} and {text}\n",
    "    # MOST IMPORTANTLY: answer the questions from {text} in such a way to write in exam\\n\n",
    "    # NOTE: try to answwer {ques} from {text} as much as possible, and bigger answer is prefered\n",
    "    # \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    prompt = f\"\"\"Given the question '{ques}', the previous chat history '{chats}', and a knowledge base '{text}':\n",
    "    1. Identify if '{ques}' is a question or an instruction. For instructions, provide an affirmative response.\n",
    "    2. If it's a question, answer it based on the information in '{text}'. Generate the answer which is suitable for an exam setting.\n",
    "    3. If '{ques}' requires information beyond '{text}', use related keywords from '{text}' to answer but don't left any question unanswered, and mention [from other sources as well] at the end.\n",
    "    4. For coding questions, include an algorithm explanation. For career guidance inquiries, offer comprehensive advice and resource links. If asked to differentiate, provide at least 7 comparison points in a table with an example.\n",
    "    5. Incorporate relevant details from the chat history '{chats}' when answering, especially if the question refers to previous discussions.\n",
    "    6. Aim to source answers primarily from '{text}', ensuring the response is detailed (answer should be big) and exam-appropriate.\n",
    "    \"\"\" \n",
    "\n",
    "    gen_config = GenerationConfig(temperature=0.7)\n",
    "    answer_text = model.generate_content(prompt,generation_config=gen_config)\n",
    "    answer = extract_text_from_response(answer_text)\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:yellow\">Question:-</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = \" Draw the star schema for previous answer \"\n",
    "passages = get_relevant_passages(ques, db, n_results=25) #i have kept the n_results more because i wanted more info to be included in my answwer\n",
    "txt = \"\"\n",
    "for passage in passages:\n",
    "    txt += passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(passages):\n",
    "    content = \"\"\n",
    "    for passage in passages:\n",
    "        content += passage + \"\\n\"\n",
    "    return content\n",
    "\n",
    "cont = list_to_string(passages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a valid employee in the EMPLOYEES table.ALTER TABLE     employeesADD CONSTRAINT  emp_manager_fk   FOREIGN KEY(manager_id)   REFERENCES employees(employee_id); a valid employee in the EMPLOYEES table.ALTER TABLE     employeesADD CONSTRAINT  emp_manager_fk   FOREIGN KEY(manager_id)   REFERENCES employees(employee_id); a valid employee in the EMPLOYEES table.ALTER TABLE     employeesADD CONSTRAINT  emp_manager_fk   FOREIGN KEY(manager_id)   REFERENCES employees(employee_id); \"lifecycleState\" : \"PUBLISHED\" ,            \"lastModifiedAt\" : 1648515427782 ,            \"visibility\" : \"PUBLIC\" ,            \"publishedAt\" : 1648515427701 ,            \"author\" : \"urn:li:organization:1234567\" ,            \"distribution\" : {                \"feedDistribution\" : \"NONE\",                \"thirdPartyDistributionChannels\" : []            },            \"content\" : {                \"carousel\" : {                    \"cards\": [                        {                            \"landingPage\" : \"http://www.linkedin.com/\" ,                            \"media\": {                                \"title\": \"first card\" ,                                \"taggedEntities\" : [],                                \"id\": \"urn:li:image:C4E22AQGX_uq7mQBfAA\"                            }                        },                        {                            \"landingPage\" : \"http://www.linkedin.com/\" ,                            \"media\": { \"lifecycleState\" : \"PUBLISHED\" ,            \"lastModifiedAt\" : 1648515427782 ,            \"visibility\" : \"PUBLIC\" ,            \"publishedAt\" : 1648515427701 ,            \"author\" : \"urn:li:organization:1234567\" ,            \"distribution\" : {                \"feedDistribution\" : \"NONE\",                \"thirdPartyDistributionChannels\" : []            },            \"content\" : {                \"carousel\" : {                    \"cards\": [                        {                            \"landingPage\" : \"http://www.linkedin.com/\" ,                            \"media\": {                                \"title\": \"first card\" ,                                \"taggedEntities\" : [],                                \"id\": \"urn:li:image:C4E22AQGX_uq7mQBfAA\"                            }                        },                        {                            \"landingPage\" : \"http://www.linkedin.com/\" ,                            \"media\": { \"lifecycleState\" : \"PUBLISHED\" ,            \"lastModifiedAt\" : 1648515427782 ,            \"visibility\" : \"PUBLIC\" ,            \"publishedAt\" : 1648515427701 ,            \"author\" : \"urn:li:organization:1234567\" ,            \"distribution\" : {                \"feedDistribution\" : \"NONE\",                \"thirdPartyDistributionChannels\" : []            },            \"content\" : {                \"carousel\" : {                    \"cards\": [                        {                            \"landingPage\" : \"http://www.linkedin.com/\" ,                            \"media\": {                                \"title\": \"first card\" ,                                \"taggedEntities\" : [],                                \"id\": \"urn:li:image:C4E22AQGX_uq7mQBfAA\"                            }                        },                        {                            \"landingPage\" : \"http://www.linkedin.com/\" ,                            \"media\": { \"lifecycleState\" : \"PUBLISHED\" ,            \"lastModifiedAt\" : 1648515427782 ,            \"visibility\" : \"PUBLIC\" ,            \"publishedAt\" : 1648515427701 ,            \"author\" : \"urn:li:organization:1234567\" ,            \"distribution\" : {                \"feedDistribution\" : \"NONE\",                \"thirdPartyDistributionChannels\" : []            },            \"content\" : {                \"carousel\" : {                    \"cards\": [                        {                            \"landingPage\" : \"http://www.linkedin.com/\" ,                            \"media\": {                                \"title\": \"first card\" ,                                \"taggedEntities\" : [],                                \"id\": \"urn:li:image:C4E22AQGX_uq7mQBfAA\"                            }                        },                        {                            \"landingPage\" : \"http://www.linkedin.com/\" ,                            \"media\": { NUMBER(6),         first_name   VARCHAR2(20),       ...       job_id       VARCHAR2(10) NOT NULL,      CONSTRAINT emp_emp_id_pk              PRIMARY KEY (EMPLOYEE_ID));The NOT NULL ConstraintEnsures that null values are not permitted for the column: NUMBER(6),         first_name   VARCHAR2(20),       ...       job_id       VARCHAR2(10) NOT NULL,      CONSTRAINT emp_emp_id_pk              PRIMARY KEY (EMPLOYEE_ID));The NOT NULL ConstraintEnsures that null values are not permitted for the column: NUMBER(6),         first_name   VARCHAR2(20),       ...       job_id       VARCHAR2(10) NOT NULL,      CONSTRAINT emp_emp_id_pk              PRIMARY KEY (EMPLOYEE_ID));The NOT NULL ConstraintEnsures that null values are not permitted for the column: }                                     }                                 },                                 \"identifiers\" : [                                     {                                         \"identifier\" :  \"https://example.com/image.jpg\" ,                                         \"index\": 0,                                         \"mediaType\" : \"image/jpeg\" ,                                         \"file\": \"urn:li:digitalmediaFile: (urn:li:digitalmediaAsset:C5603AQEuebSM6NP8wq,urn:li:digitalmediaMediaArtifa ctClass:profile-displayphoto-shrink_800_800,0)\" ,                                         \"identifierType\" : \"EXTERNAL_URL\" ,                                         \"identifierExpiresInSeconds\" :  1609372800                                     }                                 ]                             }                         ]                     }                 },                 \"firstName\" : { }                                     }                                 },                                 \"identifiers\" : [                                     {                                         \"identifier\" :  \"https://example.com/image.jpg\" ,                                         \"index\": 0,                                         \"mediaType\" : \"image/jpeg\" ,                                         \"file\": \"urn:li:digitalmediaFile: (urn:li:digitalmediaAsset:C5603AQEuebSM6NP8wq,urn:li:digitalmediaMediaArtifa ctClass:profile-displayphoto-shrink_800_800,0)\" ,                                         \"identifierType\" : \"EXTERNAL_URL\" ,                                         \"identifierExpiresInSeconds\" :  1609372800                                     }                                 ]                             }                         ]                     }                 },                 \"firstName\" : { }                                     }                                 },                                 \"identifiers\" : [                                     {                                         \"identifier\" :  \"https://example.com/image.jpg\" ,                                         \"index\": 0,                                         \"mediaType\" : \"image/jpeg\" ,                                         \"file\": \"urn:li:digitalmediaFile: (urn:li:digitalmediaAsset:C5603AQEuebSM6NP8wq,urn:li:digitalmediaMediaArtifa ctClass:profile-displayphoto-shrink_800_800,0)\" ,                                         \"identifierType\" : \"EXTERNAL_URL\" ,                                         \"identifierExpiresInSeconds\" :  1609372800                                     }                                 ]                             }                         ]                     }                 },                 \"firstName\" : { }                                     }                                 },                                 \"identifiers\" : [                                     {                                         \"identifier\" :  \"https://example.com/image.jpg\" ,                                         \"index\": 0,                                         \"mediaType\" : \"image/jpeg\" ,                                         \"file\": \"urn:li:digitalmediaFile: (urn:li:digitalmediaAsset:C5603AQEuebSM6NP8wq,urn:li:digitalmediaMediaArtifa ctClass:profile-displayphoto-shrink_800_800,0)\" ,                                         \"identifierType\" : \"EXTERNAL_URL\" ,                                         \"identifierExpiresInSeconds\" :  1609372800                                     }                                 ]                             }                         ]                     }                 },                 \"firstName\" : { idType IdType Type of this ID. All IDs of a given type must use the same format/encoding. \\xa0 idValue string An opaque ID in type-specific format. userIds \\xa0 List[T ypedId] (optional)List of ID to match. \\xa0 idType IdType Type of this ID. All IDs of a given type must use theRate Limits Add or Remove a User Schema ﾉExpand tableField NameSub- FieldType Descr iption same format/encoding. \\xa0 idValue string An opaque ID in type-specific format. firstName String (optional)A plain text string with max length 35 characters representing the first name of the contact to match e.g. Mike lastName String (optional)A plain text string with max length 35 characters representing the last name of the contact to match e.g. Smith title String (optional)A plain text string with max length 50 characters representing the title name of the contact to match e.g. Software Engineer company String (optional)A plain text string with max length 50 characters representing the company name of the contact to idType IdType Type of this ID. All IDs of a given type must use the same format/encoding. \\xa0 idValue string An opaque ID in type-specific format. userIds \\xa0 List[T ypedId] (optional)List of ID to match. \\xa0 idType IdType Type of this ID. All IDs of a given type must use theRate Limits Add or Remove a User Schema ﾉExpand tableField NameSub- FieldType Descr iption same format/encoding. \\xa0 idValue string An opaque ID in type-specific format. firstName String (optional)A plain text string with max length 35 characters representing the first name of the contact to match e.g. Mike lastName String (optional)A plain text string with max length 35 characters representing the last name of the contact to match e.g. Smith title String (optional)A plain text string with max length 50 characters representing the title name of the contact to match e.g. Software Engineer company String (optional)A plain text string with max length 50 characters representing the company name of the contact to idType IdType Type of this ID. All IDs of a given type must use the same format/encoding. \\xa0 idValue string An opaque ID in type-specific format. userIds \\xa0 List[T ypedId] (optional)List of ID to match. \\xa0 idType IdType Type of this ID. All IDs of a given type must use theRate Limits Add or Remove a User Schema ﾉExpand tableField NameSub- FieldType Descr iption same format/encoding. \\xa0 idValue string An opaque ID in type-specific format. firstName String (optional)A plain text string with max length 35 characters representing the first name of the contact to match e.g. Mike lastName String (optional)A plain text string with max length 35 characters representing the last name of the contact to match e.g. Smith title String (optional)A plain text string with max length 50 characters representing the title name of the contact to match e.g. Software Engineer company String (optional)A plain text string with max length 50 characters representing the company name of the contact to idType IdType Type of this ID. All IDs of a given type must use the same format/encoding. \\xa0 idValue string An opaque ID in type-specific format. userIds \\xa0 List[T ypedId] (optional)List of ID to match. \\xa0 idType IdType Type of this ID. All IDs of a given type must use theRate Limits Add or Remove a User Schema ﾉExpand tableField NameSub- FieldType Descr iption same format/encoding. \\xa0 idValue string An opaque ID in type-specific format. firstName String (optional)A plain text string with max length 35 characters representing the first name of the contact to match e.g. Mike lastName String (optional)A plain text string with max length 35 characters representing the last name of the contact to match e.g. Smith title String (optional)A plain text string with max length 50 characters representing the title name of the contact to match e.g. Software Engineer company String (optional)A plain text string with max length 50 characters representing the company name of the contact to transition from your earlier tools where you can extend the boundaries of traditional relational data processing.Spark Eco-System },                         \"predefinedField\" : \"FIRST_NAME\" ,                         \"responseRequired\" : true                     },                     {                         \"questionId\" : 10540,                         \"question\" : {                             \"localized\" : {                                 \"en_US\": \"Last Name\"                             }                         },                         \"responseEditable\" : true,                         \"name\": \"lastName\" ,                         \"questionDetails\" : {                             \"textQuestionDetails\" : {                                 \"maxResponseLength\" : 300                             }                         },                         \"predefinedField\" : \"LAST_NAME\" ,                         \"responseRequired\" : true                     },                     {                         \"questionId\" : 9244,                         \"question\" : { },                         \"predefinedField\" : \"FIRST_NAME\" ,                         \"responseRequired\" : true                     },                     {                         \"questionId\" : 10540,                         \"question\" : {                             \"localized\" : {                                 \"en_US\": \"Last Name\"                             }                         },                         \"responseEditable\" : true,                         \"name\": \"lastName\" ,                         \"questionDetails\" : {                             \"textQuestionDetails\" : {                                 \"maxResponseLength\" : 300                             }                         },                         \"predefinedField\" : \"LAST_NAME\" ,                         \"responseRequired\" : true                     },                     {                         \"questionId\" : 9244,                         \"question\" : { },                         \"predefinedField\" : \"FIRST_NAME\" ,                         \"responseRequired\" : true                     },                     {                         \"questionId\" : 10540,                         \"question\" : {                             \"localized\" : {                                 \"en_US\": \"Last Name\"                             }                         },                         \"responseEditable\" : true,                         \"name\": \"lastName\" ,                         \"questionDetails\" : {                             \"textQuestionDetails\" : {                                 \"maxResponseLength\" : 300                             }                         },                         \"predefinedField\" : \"LAST_NAME\" ,                         \"responseRequired\" : true                     },                     {                         \"questionId\" : 9244,                         \"question\" : { },                         \"predefinedField\" : \"FIRST_NAME\" ,                         \"responseRequired\" : true                     },                     {                         \"questionId\" : 10540,                         \"question\" : {                             \"localized\" : {                                 \"en_US\": \"Last Name\"                             }                         },                         \"responseEditable\" : true,                         \"name\": \"lastName\" ,                         \"questionDetails\" : {                             \"textQuestionDetails\" : {                                 \"maxResponseLength\" : 300                             }                         },                         \"predefinedField\" : \"LAST_NAME\" ,                         \"responseRequired\" : true                     },                     {                         \"questionId\" : 9244,                         \"question\" : { \"publishedAt\" : 1634817394721 ,       \"author\" : \"urn:li:organization:2414183\" ,       \"distribution\" : {         \"feedDistribution\" : \"MAIN_FEED\" ,         \"thirdPartyDistributionChannels\" : []       },       \"content\" : {},       \"lifecycleStateInfo\" : {         \"isEditedByAuthor\" : false       },       \"isReshareDisabledByAuthor\" : false,       \"createdAt\" : 1634817394721 ,       \"id\": \"urn:li:share:6856921137721544704\" ,       \"commentary\" : \"\"     },     {       \"lifecycleState\" : \"PUBLISHED\" ,       \"lastModifiedAt\" : 1634814069101 ,       \"visibility\" : \"PUBLIC\" ,       \"publishedAt\" : 1634813460041 ,       \"author\" : \"urn:li:organization:2414183\" ,       \"distribution\" : {         \"feedDistribution\" : \"MAIN_FEED\" ,         \"thirdPartyDistributionChannels\" : []       },       \"lifecycleStateInfo\" : {         \"isEditedByAuthor\" : false       },       \"isReshareDisabledByAuthor\" : false,       \"createdAt\" : 1634813460041 ,       \"id\": \"urn:li:share:6856904634360066048\" , \"publishedAt\" : 1634817394721 ,       \"author\" : \"urn:li:organization:2414183\" ,       \"distribution\" : {         \"feedDistribution\" : \"MAIN_FEED\" ,         \"thirdPartyDistributionChannels\" : []       },       \"content\" : {},       \"lifecycleStateInfo\" : {         \"isEditedByAuthor\" : false       },       \"isReshareDisabledByAuthor\" : false,       \"createdAt\" : 1634817394721 ,       \"id\": \"urn:li:share:6856921137721544704\" ,       \"commentary\" : \"\"     },     {       \"lifecycleState\" : \"PUBLISHED\" ,       \"lastModifiedAt\" : 1634814069101 ,       \"visibility\" : \"PUBLIC\" ,       \"publishedAt\" : 1634813460041 ,       \"author\" : \"urn:li:organization:2414183\" ,       \"distribution\" : {         \"feedDistribution\" : \"MAIN_FEED\" ,         \"thirdPartyDistributionChannels\" : []       },       \"lifecycleStateInfo\" : {         \"isEditedByAuthor\" : false       },       \"isReshareDisabledByAuthor\" : false,       \"createdAt\" : 1634813460041 ,       \"id\": \"urn:li:share:6856904634360066048\" , '"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont = cont.replace('Module :1 Introduction to Big Data and Hadoop',' ') #this is subjective for this data\n",
    "cont = cont.replace('Lecture 1: Introduction to Big Data3Lecture 2: Hadoop Ecosystem 58  Lecture 3: Introduction to spark3Lecture 4: Hadoop vs Spark58',' ') #this is subjective for this data\n",
    "cont = cont.replace('\\n',' ')  #this is subjective for this data\n",
    "cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = make_prompt(ques, cont, chats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Exam-Appropriate Response:**\\n\\n**Question:** Draw the star schema for the previous answer.\\n\\n**Answer:**\\n\\nA star schema is a data warehouse design pattern that is optimized for querying data from multiple dimensions. It consists of a fact table and multiple dimension tables. The fact table contains the main business data, while the dimension tables contain the descriptive attributes for each dimension.\\n\\nIn the previous answer, the fact table is the `Sales` table, which contains the following columns:\\n\\n* `Product_ID`\\n* `Order_ID`\\n* `Customer_ID`\\n* `Employee_ID`\\n* `Total_Sales`\\n* `Quantity_Sold`\\n* `Discount`\\n\\nThe dimension tables are:\\n\\n* `Product` table, which contains the following columns:\\n    * `Product_ID`\\n    * `Product_Name`\\n    * `Product_Category`\\n    * `Unit_Price`\\n* `Customer` table, which contains the following columns:\\n    * `Customer_ID`\\n    * `Customer_Name`\\n    * `Customer_Address`\\n    * `Customer_City`\\n    * `Customer_State`\\n    * `Customer_Zip_Code`\\n* `Employee` table, which contains the following columns:\\n    * `Employee_ID`\\n    * `Employee_Name`\\n    * `Employee_Title`\\n    * `Employee_Department`\\n    * `Employee_Region`\\n* `Time` table, which contains the following columns:\\n    * `Order_ID`\\n    * `Order_Date`\\n    * `Order_Year`\\n    * `Order_Quarter`\\n    * `Order_Month`\\n\\nThe relationships between the tables are as follows:\\n\\n* The `Sales` table is related to the `Product` table by the `Product_ID` column.\\n* The `Sales` table is related to the `Customer` table by the `Customer_ID` column.\\n* The `Sales` table is related to the `Employee` table by the `Employee_ID` column.\\n* The `Sales` table is related to the `Time` table by the `Order_ID` column.\\n\\nThe star schema for the previous answer is shown below:\\n\\n```\\nSales\\n| Product_ID | Order_ID | Customer_ID | Employee_ID | Total_Sales | Quantity_Sold | Discount |\\n|---|---|---|---|---|---|---|\\n| 1 | 1 | 1 | 1 | 100.00 | 10 | 0.10 |\\n| 2 | 2 | 2 | 2 | 200.00 | 20 | 0.15 |\\n| 3 | 3 | 3 | 3 | 300.00 | 30 | 0.20 |\\n| 4 | 4 | 4 | 4 | 400.00 | 40 | 0.25 |\\n| 5 | 5 | 5 | 5 | 500.00 | 50 | 0.30 |\\n\\nProduct\\n| Product_ID | Product_Name | Product_Category | Unit_Price |\\n|---|---|---|---|\\n| 1 | Product A | Category A | 10.00 |\\n| 2 | Product B | Category B | 20.00 |\\n| 3 | Product C | Category C | 30.00 |\\n| 4 | Product D | Category D | 40.00 |\\n| 5 | Product E | Category E | 50.00 |\\n\\nCustomer\\n| Customer_ID | Customer_Name | Customer_Address | Customer_City | Customer_State | Customer_Zip_Code |\\n|---|---|---|---|---|---|\\n| 1 | Customer A | 123 Main Street | Anytown | CA | 12345 |\\n| 2 | Customer B | 456 Elm Street | Hometown | NY | 54321 |\\n| 3 | Customer C | 789 Oak Street | Cityville | TX | 67890 |\\n| 4 | Customer D | 1011 Pine Street | Woodville | FL | 23456 |\\n| 5 | Customer E | 1213 Maple Street | Springville | IL | 34567 |\\n\\nEmployee\\n| Employee_ID | Employee_Name | Employee_Title | Employee_Department | Employee_Region |\\n|---|---|---|---|---|\\n| 1 | Employee A | Salesperson | Sales | East |\\n| 2 | Employee B | Manager | Management | West |\\n| 3 | Employee C | Analyst | Analytics | Central |\\n| 4 | Employee D | Developer | Development | South |\\n| 5 | Employee E | Tester | Testing | North |\\n\\nTime\\n| Order_ID | Order_Date | Order_Year | Order_Quarter | Order_Month |\\n|---|---|---|---|---|\\n| 1 | 2023-01-01 | 2023 | 1 | 1 |\\n| 2 | 2023-02-01 | 2023 | 1 | 2 |\\n| 3 | 2023-03-01 | 2023 | 1 | 3 |\\n| 4 | 2023-04-01 | 2023 | 2 | 1 |\\n| 5 | 2023-05-01 | 2023 | 2 | 2 |\\n```\\n\\nThis star schema can be used to answer queries such as:\\n\\n* What is the total sales for each product?\\n* What is the average sales price for each product category?\\n* Who are the top 10 customers in terms of total sales?\\n* What is the sales trend over time?'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add a new Q&A pair\n",
    "def chat_history(ques, answer, chats):\n",
    "    next_id = len(chats) + 1\n",
    "    chats[next_id] = {\"question\": ques, \"answer\": answer}\n",
    "if len(chats):\n",
    "    if (chats[len(chats)]['question']!=ques or chats[len(chats)]['question']!=ques):\n",
    "        chat_history(ques=ques, answer=answer, chats=chats)\n",
    "    else:\n",
    "        print(\"same\")\n",
    "else:\n",
    "    chat_history(ques=ques, answer=answer, chats=chats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:yellow\">Answer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Exam-Appropriate Response:**\n",
       "\n",
       "**Question:** Draw the star schema for the previous answer.\n",
       "\n",
       "**Answer:**\n",
       "\n",
       "A star schema is a data warehouse design pattern that is optimized for querying data from multiple dimensions. It consists of a fact table and multiple dimension tables. The fact table contains the main business data, while the dimension tables contain the descriptive attributes for each dimension.\n",
       "\n",
       "In the previous answer, the fact table is the `Sales` table, which contains the following columns:\n",
       "\n",
       "* `Product_ID`\n",
       "* `Order_ID`\n",
       "* `Customer_ID`\n",
       "* `Employee_ID`\n",
       "* `Total_Sales`\n",
       "* `Quantity_Sold`\n",
       "* `Discount`\n",
       "\n",
       "The dimension tables are:\n",
       "\n",
       "* `Product` table, which contains the following columns:\n",
       "    * `Product_ID`\n",
       "    * `Product_Name`\n",
       "    * `Product_Category`\n",
       "    * `Unit_Price`\n",
       "* `Customer` table, which contains the following columns:\n",
       "    * `Customer_ID`\n",
       "    * `Customer_Name`\n",
       "    * `Customer_Address`\n",
       "    * `Customer_City`\n",
       "    * `Customer_State`\n",
       "    * `Customer_Zip_Code`\n",
       "* `Employee` table, which contains the following columns:\n",
       "    * `Employee_ID`\n",
       "    * `Employee_Name`\n",
       "    * `Employee_Title`\n",
       "    * `Employee_Department`\n",
       "    * `Employee_Region`\n",
       "* `Time` table, which contains the following columns:\n",
       "    * `Order_ID`\n",
       "    * `Order_Date`\n",
       "    * `Order_Year`\n",
       "    * `Order_Quarter`\n",
       "    * `Order_Month`\n",
       "\n",
       "The relationships between the tables are as follows:\n",
       "\n",
       "* The `Sales` table is related to the `Product` table by the `Product_ID` column.\n",
       "* The `Sales` table is related to the `Customer` table by the `Customer_ID` column.\n",
       "* The `Sales` table is related to the `Employee` table by the `Employee_ID` column.\n",
       "* The `Sales` table is related to the `Time` table by the `Order_ID` column.\n",
       "\n",
       "The star schema for the previous answer is shown below:\n",
       "\n",
       "```\n",
       "Sales\n",
       "| Product_ID | Order_ID | Customer_ID | Employee_ID | Total_Sales | Quantity_Sold | Discount |\n",
       "|---|---|---|---|---|---|---|\n",
       "| 1 | 1 | 1 | 1 | 100.00 | 10 | 0.10 |\n",
       "| 2 | 2 | 2 | 2 | 200.00 | 20 | 0.15 |\n",
       "| 3 | 3 | 3 | 3 | 300.00 | 30 | 0.20 |\n",
       "| 4 | 4 | 4 | 4 | 400.00 | 40 | 0.25 |\n",
       "| 5 | 5 | 5 | 5 | 500.00 | 50 | 0.30 |\n",
       "\n",
       "Product\n",
       "| Product_ID | Product_Name | Product_Category | Unit_Price |\n",
       "|---|---|---|---|\n",
       "| 1 | Product A | Category A | 10.00 |\n",
       "| 2 | Product B | Category B | 20.00 |\n",
       "| 3 | Product C | Category C | 30.00 |\n",
       "| 4 | Product D | Category D | 40.00 |\n",
       "| 5 | Product E | Category E | 50.00 |\n",
       "\n",
       "Customer\n",
       "| Customer_ID | Customer_Name | Customer_Address | Customer_City | Customer_State | Customer_Zip_Code |\n",
       "|---|---|---|---|---|---|\n",
       "| 1 | Customer A | 123 Main Street | Anytown | CA | 12345 |\n",
       "| 2 | Customer B | 456 Elm Street | Hometown | NY | 54321 |\n",
       "| 3 | Customer C | 789 Oak Street | Cityville | TX | 67890 |\n",
       "| 4 | Customer D | 1011 Pine Street | Woodville | FL | 23456 |\n",
       "| 5 | Customer E | 1213 Maple Street | Springville | IL | 34567 |\n",
       "\n",
       "Employee\n",
       "| Employee_ID | Employee_Name | Employee_Title | Employee_Department | Employee_Region |\n",
       "|---|---|---|---|---|\n",
       "| 1 | Employee A | Salesperson | Sales | East |\n",
       "| 2 | Employee B | Manager | Management | West |\n",
       "| 3 | Employee C | Analyst | Analytics | Central |\n",
       "| 4 | Employee D | Developer | Development | South |\n",
       "| 5 | Employee E | Tester | Testing | North |\n",
       "\n",
       "Time\n",
       "| Order_ID | Order_Date | Order_Year | Order_Quarter | Order_Month |\n",
       "|---|---|---|---|---|\n",
       "| 1 | 2023-01-01 | 2023 | 1 | 1 |\n",
       "| 2 | 2023-02-01 | 2023 | 1 | 2 |\n",
       "| 3 | 2023-03-01 | 2023 | 1 | 3 |\n",
       "| 4 | 2023-04-01 | 2023 | 2 | 1 |\n",
       "| 5 | 2023-05-01 | 2023 | 2 | 2 |\n",
       "```\n",
       "\n",
       "This star schema can be used to answer queries such as:\n",
       "\n",
       "* What is the total sales for each product?\n",
       "* What is the average sales price for each product category?\n",
       "* Who are the top 10 customers in terms of total sales?\n",
       "* What is the sales trend over time?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': ' Write a set of SQL queries to implement Star Schema ', 'answer': \"**SQL Queries to Implement Star Schema:**\\n\\n```sql\\n-- Create the fact table\\nCREATE TABLE FactSales (\\n  Product_ID INT NOT NULL,\\n  Order_ID INT NOT NULL,\\n  Customer_ID INT NOT NULL,\\n  Employee_ID INT NOT NULL,\\n  Total_Sales DECIMAL(10, 2) NOT NULL,\\n  Quantity_Sold INT NOT NULL,\\n  Discount DECIMAL(5, 2) NOT NULL,\\n  PRIMARY KEY (Product_ID, Order_ID)\\n);\\n\\n-- Create the dimension tables\\nCREATE TABLE DimProduct (\\n  Product_ID INT NOT NULL,\\n  Product_Name VARCHAR(50) NOT NULL,\\n  Product_Category VARCHAR(50) NOT NULL,\\n  Unit_Price DECIMAL(10, 2) NOT NULL,\\n  PRIMARY KEY (Product_ID)\\n);\\n\\nCREATE TABLE DimCustomer (\\n  Customer_ID INT NOT NULL,\\n  Customer_Name VARCHAR(50) NOT NULL,\\n  Customer_Address VARCHAR(100) NOT NULL,\\n  Customer_City VARCHAR(50) NOT NULL,\\n  Customer_State VARCHAR(2) NOT NULL,\\n  Customer_Zip_Code VARCHAR(10) NOT NULL,\\n  PRIMARY KEY (Customer_ID)\\n);\\n\\nCREATE TABLE DimEmployee (\\n  Employee_ID INT NOT NULL,\\n  Employee_Name VARCHAR(50) NOT NULL,\\n  Employee_Title VARCHAR(50) NOT NULL,\\n  Employee_Department VARCHAR(50) NOT NULL,\\n  Employee_Region VARCHAR(50) NOT NULL,\\n  PRIMARY KEY (Employee_ID)\\n);\\n\\nCREATE TABLE DimTime (\\n  Order_ID INT NOT NULL,\\n  Order_Date DATE NOT NULL,\\n  Order_Year INT NOT NULL,\\n  Order_Quarter INT NOT NULL,\\n  Order_Month INT NOT NULL,\\n  PRIMARY KEY (Order_ID)\\n);\\n\\n-- Insert data into the tables\\nINSERT INTO FactSales (Product_ID, Order_ID, Customer_ID, Employee_ID, Total_Sales, Quantity_Sold, Discount)\\nVALUES\\n  (1, 1, 1, 1, 100.00, 10, 0.10),\\n  (2, 2, 2, 2, 200.00, 20, 0.15),\\n  (3, 3, 3, 3, 300.00, 30, 0.20),\\n  (4, 4, 4, 4, 400.00, 40, 0.25),\\n  (5, 5, 5, 5, 500.00, 50, 0.30);\\n\\nINSERT INTO DimProduct (Product_ID, Product_Name, Product_Category, Unit_Price)\\nVALUES\\n  (1, 'Product A', 'Category A', 10.00),\\n  (2, 'Product B', 'Category B', 20.00),\\n  (3, 'Product C', 'Category C', 30.00),\\n  (4, 'Product D', 'Category D', 40.00),\\n  (5, 'Product E', 'Category E', 50.00);\\n\\nINSERT INTO DimCustomer (Customer_ID, Customer_Name, Customer_Address, Customer_City, Customer_State, Customer_Zip_Code)\\nVALUES\\n  (1, 'Customer A', '123 Main Street', 'Anytown', 'CA', '12345'),\\n  (2, 'Customer B', '456 Elm Street', 'Hometown', 'NY', '54321'),\\n  (3, 'Customer C', '789 Oak Street', 'Cityville', 'TX', '67890'),\\n  (4, 'Customer D', '1011 Pine Street', 'Woodville', 'FL', '23456'),\\n  (5, 'Customer E', '1213 Maple Street', 'Springville', 'IL', '34567');\\n\\nINSERT INTO DimEmployee (Employee_ID, Employee_Name, Employee_Title, Employee_Department, Employee_Region)\\nVALUES\\n  (1, 'Employee A', 'Salesperson', 'Sales', 'East'),\\n  (2, 'Employee B', 'Manager', 'Management', 'West'),\\n  (3, 'Employee C', 'Analyst', 'Analytics', 'Central'),\\n  (4, 'Employee D', 'Developer', 'Development', 'South'),\\n  (5, 'Employee E', 'Tester', 'Testing', 'North');\\n\\nINSERT INTO DimTime (Order_ID, Order_Date, Order_Year, Order_Quarter, Order_Month)\\nVALUES\\n  (1, '2023-01-01', 2023, 1, 1),\\n  (2, '2023-02-01', 2023, 1, 2),\\n  (3, '2023-03-01', 2023, 1, 3),\\n  (4, '2023-04-01', 2023, 2, 1),\\n  (5, '2023-05-01', 2023, 2, 2);\\n```\\n\\n**Explanation:**\\n\\nThe above SQL queries create a star schema with five dimension tables (DimProduct, DimCustomer, DimEmployee, DimTime) and one fact table (FactSales). The fact table contains the sales data, while the dimension tables contain the descriptive attributes for each dimension. The primary keys of the dimension tables are foreign keys in the fact table, which establishes the relationships between the tables.\\n\\nThe data in the tables is inserted using the `INSERT` statement. The data is representative of a sales system, with products, customers, employees, and time dimensions.\\n\\nTo query the data in the star schema, you can use SQL queries that join the fact table with the dimension tables. For example, the following query returns the total sales for each product:\\n\\n```sql\\nSELECT\\n  Product_Name,\\n  SUM(Total_Sales) AS TotalSales\\nFROM FactSales\\nJOIN DimProduct\\n  ON FactSales.Product_ID = DimProduct.Product_ID\\nGROUP BY\\n  Product_Name;\\n```\"}\n",
      "{'question': ' Draw the star schema for previous answer ', 'answer': '**Exam-Appropriate Response:**\\n\\n**Question:** Draw the star schema for the previous answer.\\n\\n**Answer:**\\n\\nA star schema is a data warehouse design pattern that is optimized for querying data from multiple dimensions. It consists of a fact table and multiple dimension tables. The fact table contains the main business data, while the dimension tables contain the descriptive attributes for each dimension.\\n\\nIn the previous answer, the fact table is the `Sales` table, which contains the following columns:\\n\\n* `Product_ID`\\n* `Order_ID`\\n* `Customer_ID`\\n* `Employee_ID`\\n* `Total_Sales`\\n* `Quantity_Sold`\\n* `Discount`\\n\\nThe dimension tables are:\\n\\n* `Product` table, which contains the following columns:\\n    * `Product_ID`\\n    * `Product_Name`\\n    * `Product_Category`\\n    * `Unit_Price`\\n* `Customer` table, which contains the following columns:\\n    * `Customer_ID`\\n    * `Customer_Name`\\n    * `Customer_Address`\\n    * `Customer_City`\\n    * `Customer_State`\\n    * `Customer_Zip_Code`\\n* `Employee` table, which contains the following columns:\\n    * `Employee_ID`\\n    * `Employee_Name`\\n    * `Employee_Title`\\n    * `Employee_Department`\\n    * `Employee_Region`\\n* `Time` table, which contains the following columns:\\n    * `Order_ID`\\n    * `Order_Date`\\n    * `Order_Year`\\n    * `Order_Quarter`\\n    * `Order_Month`\\n\\nThe relationships between the tables are as follows:\\n\\n* The `Sales` table is related to the `Product` table by the `Product_ID` column.\\n* The `Sales` table is related to the `Customer` table by the `Customer_ID` column.\\n* The `Sales` table is related to the `Employee` table by the `Employee_ID` column.\\n* The `Sales` table is related to the `Time` table by the `Order_ID` column.\\n\\nThe star schema for the previous answer is shown below:\\n\\n```\\nSales\\n| Product_ID | Order_ID | Customer_ID | Employee_ID | Total_Sales | Quantity_Sold | Discount |\\n|---|---|---|---|---|---|---|\\n| 1 | 1 | 1 | 1 | 100.00 | 10 | 0.10 |\\n| 2 | 2 | 2 | 2 | 200.00 | 20 | 0.15 |\\n| 3 | 3 | 3 | 3 | 300.00 | 30 | 0.20 |\\n| 4 | 4 | 4 | 4 | 400.00 | 40 | 0.25 |\\n| 5 | 5 | 5 | 5 | 500.00 | 50 | 0.30 |\\n\\nProduct\\n| Product_ID | Product_Name | Product_Category | Unit_Price |\\n|---|---|---|---|\\n| 1 | Product A | Category A | 10.00 |\\n| 2 | Product B | Category B | 20.00 |\\n| 3 | Product C | Category C | 30.00 |\\n| 4 | Product D | Category D | 40.00 |\\n| 5 | Product E | Category E | 50.00 |\\n\\nCustomer\\n| Customer_ID | Customer_Name | Customer_Address | Customer_City | Customer_State | Customer_Zip_Code |\\n|---|---|---|---|---|---|\\n| 1 | Customer A | 123 Main Street | Anytown | CA | 12345 |\\n| 2 | Customer B | 456 Elm Street | Hometown | NY | 54321 |\\n| 3 | Customer C | 789 Oak Street | Cityville | TX | 67890 |\\n| 4 | Customer D | 1011 Pine Street | Woodville | FL | 23456 |\\n| 5 | Customer E | 1213 Maple Street | Springville | IL | 34567 |\\n\\nEmployee\\n| Employee_ID | Employee_Name | Employee_Title | Employee_Department | Employee_Region |\\n|---|---|---|---|---|\\n| 1 | Employee A | Salesperson | Sales | East |\\n| 2 | Employee B | Manager | Management | West |\\n| 3 | Employee C | Analyst | Analytics | Central |\\n| 4 | Employee D | Developer | Development | South |\\n| 5 | Employee E | Tester | Testing | North |\\n\\nTime\\n| Order_ID | Order_Date | Order_Year | Order_Quarter | Order_Month |\\n|---|---|---|---|---|\\n| 1 | 2023-01-01 | 2023 | 1 | 1 |\\n| 2 | 2023-02-01 | 2023 | 1 | 2 |\\n| 3 | 2023-03-01 | 2023 | 1 | 3 |\\n| 4 | 2023-04-01 | 2023 | 2 | 1 |\\n| 5 | 2023-05-01 | 2023 | 2 | 2 |\\n```\\n\\nThis star schema can be used to answer queries such as:\\n\\n* What is the total sales for each product?\\n* What is the average sales price for each product category?\\n* Who are the top 10 customers in terms of total sales?\\n* What is the sales trend over time?'}\n",
      "Length of chats is: 3\n"
     ]
    }
   ],
   "source": [
    "if len(chats)>1:\n",
    "    print(chats[len(chats)-1])\n",
    "print(chats[len(chats)])\n",
    "print(f\"Length of chats is: {len(chats)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Differentiate between Data Node and Name Node**\n",
       "\n",
       "| Feature | Data Node | Name Node |\n",
       "|---|---|---|\n",
       "| Purpose | Stores the actual data | Manages the file system namespace and regulates access to data |\n",
       "| Location | On every machine in the Hadoop cluster | On the master node |\n",
       "| Number of instances | Multiple | Single |\n",
       "| Data storage | Data is stored in blocks on the local file system | Maintains a metadata table that tracks the location of data blocks |\n",
       "| Data access | Applications read and write data directly from and to Data Nodes | Applications interact with the Name Node to determine the location of data blocks |\n",
       "| Fault tolerance | Data is replicated across multiple Data Nodes | Maintains a backup of the metadata table on a secondary Name Node |\n",
       "| Scalability | Can be easily scaled by adding more Data Nodes to the cluster | More difficult to scale, as it is a single point of failure |\n",
       "| Performance | Can be optimized for specific workloads by tuning the local file system | Can be optimized for specific workloads by tuning the metadata management system |\n",
       "\n",
       "**Example:**\n",
       "\n",
       "In a Hadoop cluster, the Name Node stores the metadata for the file \"/user/hadoop/input.txt\". This metadata includes the location of the data blocks that make up the file. When an application wants to read the file, it first contacts the Name Node to get the location of the data blocks. The application then reads the data directly from the Data Nodes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(chats[1]['answer'])\n",
    "# n = len(chats)\n",
    "# id = 1\n",
    "# while (id<=n):\n",
    "#     Markdown(chats[id]['question'])\n",
    "#     Markdown(chats[id]['answer'])\n",
    "#     id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'question': ' Differentiate between Data Node and Name Node ',\n",
       "  'answer': '**Differentiate between Data Node and Name Node**\\n\\n| Feature | Data Node | Name Node |\\n|---|---|---|\\n| Purpose | Stores the actual data | Manages the file system namespace and regulates access to data |\\n| Location | On every machine in the Hadoop cluster | On the master node |\\n| Number of instances | Multiple | Single |\\n| Data storage | Data is stored in blocks on the local file system | Maintains a metadata table that tracks the location of data blocks |\\n| Data access | Applications read and write data directly from and to Data Nodes | Applications interact with the Name Node to determine the location of data blocks |\\n| Fault tolerance | Data is replicated across multiple Data Nodes | Maintains a backup of the metadata table on a secondary Name Node |\\n| Scalability | Can be easily scaled by adding more Data Nodes to the cluster | More difficult to scale, as it is a single point of failure |\\n| Performance | Can be optimized for specific workloads by tuning the local file system | Can be optimized for specific workloads by tuning the metadata management system |\\n\\n**Example:**\\n\\nIn a Hadoop cluster, the Name Node stores the metadata for the file \"/user/hadoop/input.txt\". This metadata includes the location of the data blocks that make up the file. When an application wants to read the file, it first contacts the Name Node to get the location of the data blocks. The application then reads the data directly from the Data Nodes.'},\n",
       " 2: {'question': ' Write a set of SQL queries to implement Star Schema ',\n",
       "  'answer': \"**SQL Queries to Implement Star Schema:**\\n\\n```sql\\n-- Create the fact table\\nCREATE TABLE FactSales (\\n  Product_ID INT NOT NULL,\\n  Order_ID INT NOT NULL,\\n  Customer_ID INT NOT NULL,\\n  Employee_ID INT NOT NULL,\\n  Total_Sales DECIMAL(10, 2) NOT NULL,\\n  Quantity_Sold INT NOT NULL,\\n  Discount DECIMAL(5, 2) NOT NULL,\\n  PRIMARY KEY (Product_ID, Order_ID)\\n);\\n\\n-- Create the dimension tables\\nCREATE TABLE DimProduct (\\n  Product_ID INT NOT NULL,\\n  Product_Name VARCHAR(50) NOT NULL,\\n  Product_Category VARCHAR(50) NOT NULL,\\n  Unit_Price DECIMAL(10, 2) NOT NULL,\\n  PRIMARY KEY (Product_ID)\\n);\\n\\nCREATE TABLE DimCustomer (\\n  Customer_ID INT NOT NULL,\\n  Customer_Name VARCHAR(50) NOT NULL,\\n  Customer_Address VARCHAR(100) NOT NULL,\\n  Customer_City VARCHAR(50) NOT NULL,\\n  Customer_State VARCHAR(2) NOT NULL,\\n  Customer_Zip_Code VARCHAR(10) NOT NULL,\\n  PRIMARY KEY (Customer_ID)\\n);\\n\\nCREATE TABLE DimEmployee (\\n  Employee_ID INT NOT NULL,\\n  Employee_Name VARCHAR(50) NOT NULL,\\n  Employee_Title VARCHAR(50) NOT NULL,\\n  Employee_Department VARCHAR(50) NOT NULL,\\n  Employee_Region VARCHAR(50) NOT NULL,\\n  PRIMARY KEY (Employee_ID)\\n);\\n\\nCREATE TABLE DimTime (\\n  Order_ID INT NOT NULL,\\n  Order_Date DATE NOT NULL,\\n  Order_Year INT NOT NULL,\\n  Order_Quarter INT NOT NULL,\\n  Order_Month INT NOT NULL,\\n  PRIMARY KEY (Order_ID)\\n);\\n\\n-- Insert data into the tables\\nINSERT INTO FactSales (Product_ID, Order_ID, Customer_ID, Employee_ID, Total_Sales, Quantity_Sold, Discount)\\nVALUES\\n  (1, 1, 1, 1, 100.00, 10, 0.10),\\n  (2, 2, 2, 2, 200.00, 20, 0.15),\\n  (3, 3, 3, 3, 300.00, 30, 0.20),\\n  (4, 4, 4, 4, 400.00, 40, 0.25),\\n  (5, 5, 5, 5, 500.00, 50, 0.30);\\n\\nINSERT INTO DimProduct (Product_ID, Product_Name, Product_Category, Unit_Price)\\nVALUES\\n  (1, 'Product A', 'Category A', 10.00),\\n  (2, 'Product B', 'Category B', 20.00),\\n  (3, 'Product C', 'Category C', 30.00),\\n  (4, 'Product D', 'Category D', 40.00),\\n  (5, 'Product E', 'Category E', 50.00);\\n\\nINSERT INTO DimCustomer (Customer_ID, Customer_Name, Customer_Address, Customer_City, Customer_State, Customer_Zip_Code)\\nVALUES\\n  (1, 'Customer A', '123 Main Street', 'Anytown', 'CA', '12345'),\\n  (2, 'Customer B', '456 Elm Street', 'Hometown', 'NY', '54321'),\\n  (3, 'Customer C', '789 Oak Street', 'Cityville', 'TX', '67890'),\\n  (4, 'Customer D', '1011 Pine Street', 'Woodville', 'FL', '23456'),\\n  (5, 'Customer E', '1213 Maple Street', 'Springville', 'IL', '34567');\\n\\nINSERT INTO DimEmployee (Employee_ID, Employee_Name, Employee_Title, Employee_Department, Employee_Region)\\nVALUES\\n  (1, 'Employee A', 'Salesperson', 'Sales', 'East'),\\n  (2, 'Employee B', 'Manager', 'Management', 'West'),\\n  (3, 'Employee C', 'Analyst', 'Analytics', 'Central'),\\n  (4, 'Employee D', 'Developer', 'Development', 'South'),\\n  (5, 'Employee E', 'Tester', 'Testing', 'North');\\n\\nINSERT INTO DimTime (Order_ID, Order_Date, Order_Year, Order_Quarter, Order_Month)\\nVALUES\\n  (1, '2023-01-01', 2023, 1, 1),\\n  (2, '2023-02-01', 2023, 1, 2),\\n  (3, '2023-03-01', 2023, 1, 3),\\n  (4, '2023-04-01', 2023, 2, 1),\\n  (5, '2023-05-01', 2023, 2, 2);\\n```\\n\\n**Explanation:**\\n\\nThe above SQL queries create a star schema with five dimension tables (DimProduct, DimCustomer, DimEmployee, DimTime) and one fact table (FactSales). The fact table contains the sales data, while the dimension tables contain the descriptive attributes for each dimension. The primary keys of the dimension tables are foreign keys in the fact table, which establishes the relationships between the tables.\\n\\nThe data in the tables is inserted using the `INSERT` statement. The data is representative of a sales system, with products, customers, employees, and time dimensions.\\n\\nTo query the data in the star schema, you can use SQL queries that join the fact table with the dimension tables. For example, the following query returns the total sales for each product:\\n\\n```sql\\nSELECT\\n  Product_Name,\\n  SUM(Total_Sales) AS TotalSales\\nFROM FactSales\\nJOIN DimProduct\\n  ON FactSales.Product_ID = DimProduct.Product_ID\\nGROUP BY\\n  Product_Name;\\n```\"},\n",
       " 3: {'question': ' Draw the star schema for previous answer ',\n",
       "  'answer': '**Exam-Appropriate Response:**\\n\\n**Question:** Draw the star schema for the previous answer.\\n\\n**Answer:**\\n\\nA star schema is a data warehouse design pattern that is optimized for querying data from multiple dimensions. It consists of a fact table and multiple dimension tables. The fact table contains the main business data, while the dimension tables contain the descriptive attributes for each dimension.\\n\\nIn the previous answer, the fact table is the `Sales` table, which contains the following columns:\\n\\n* `Product_ID`\\n* `Order_ID`\\n* `Customer_ID`\\n* `Employee_ID`\\n* `Total_Sales`\\n* `Quantity_Sold`\\n* `Discount`\\n\\nThe dimension tables are:\\n\\n* `Product` table, which contains the following columns:\\n    * `Product_ID`\\n    * `Product_Name`\\n    * `Product_Category`\\n    * `Unit_Price`\\n* `Customer` table, which contains the following columns:\\n    * `Customer_ID`\\n    * `Customer_Name`\\n    * `Customer_Address`\\n    * `Customer_City`\\n    * `Customer_State`\\n    * `Customer_Zip_Code`\\n* `Employee` table, which contains the following columns:\\n    * `Employee_ID`\\n    * `Employee_Name`\\n    * `Employee_Title`\\n    * `Employee_Department`\\n    * `Employee_Region`\\n* `Time` table, which contains the following columns:\\n    * `Order_ID`\\n    * `Order_Date`\\n    * `Order_Year`\\n    * `Order_Quarter`\\n    * `Order_Month`\\n\\nThe relationships between the tables are as follows:\\n\\n* The `Sales` table is related to the `Product` table by the `Product_ID` column.\\n* The `Sales` table is related to the `Customer` table by the `Customer_ID` column.\\n* The `Sales` table is related to the `Employee` table by the `Employee_ID` column.\\n* The `Sales` table is related to the `Time` table by the `Order_ID` column.\\n\\nThe star schema for the previous answer is shown below:\\n\\n```\\nSales\\n| Product_ID | Order_ID | Customer_ID | Employee_ID | Total_Sales | Quantity_Sold | Discount |\\n|---|---|---|---|---|---|---|\\n| 1 | 1 | 1 | 1 | 100.00 | 10 | 0.10 |\\n| 2 | 2 | 2 | 2 | 200.00 | 20 | 0.15 |\\n| 3 | 3 | 3 | 3 | 300.00 | 30 | 0.20 |\\n| 4 | 4 | 4 | 4 | 400.00 | 40 | 0.25 |\\n| 5 | 5 | 5 | 5 | 500.00 | 50 | 0.30 |\\n\\nProduct\\n| Product_ID | Product_Name | Product_Category | Unit_Price |\\n|---|---|---|---|\\n| 1 | Product A | Category A | 10.00 |\\n| 2 | Product B | Category B | 20.00 |\\n| 3 | Product C | Category C | 30.00 |\\n| 4 | Product D | Category D | 40.00 |\\n| 5 | Product E | Category E | 50.00 |\\n\\nCustomer\\n| Customer_ID | Customer_Name | Customer_Address | Customer_City | Customer_State | Customer_Zip_Code |\\n|---|---|---|---|---|---|\\n| 1 | Customer A | 123 Main Street | Anytown | CA | 12345 |\\n| 2 | Customer B | 456 Elm Street | Hometown | NY | 54321 |\\n| 3 | Customer C | 789 Oak Street | Cityville | TX | 67890 |\\n| 4 | Customer D | 1011 Pine Street | Woodville | FL | 23456 |\\n| 5 | Customer E | 1213 Maple Street | Springville | IL | 34567 |\\n\\nEmployee\\n| Employee_ID | Employee_Name | Employee_Title | Employee_Department | Employee_Region |\\n|---|---|---|---|---|\\n| 1 | Employee A | Salesperson | Sales | East |\\n| 2 | Employee B | Manager | Management | West |\\n| 3 | Employee C | Analyst | Analytics | Central |\\n| 4 | Employee D | Developer | Development | South |\\n| 5 | Employee E | Tester | Testing | North |\\n\\nTime\\n| Order_ID | Order_Date | Order_Year | Order_Quarter | Order_Month |\\n|---|---|---|---|---|\\n| 1 | 2023-01-01 | 2023 | 1 | 1 |\\n| 2 | 2023-02-01 | 2023 | 1 | 2 |\\n| 3 | 2023-03-01 | 2023 | 1 | 3 |\\n| 4 | 2023-04-01 | 2023 | 2 | 1 |\\n| 5 | 2023-05-01 | 2023 | 2 | 2 |\\n```\\n\\nThis star schema can be used to answer queries such as:\\n\\n* What is the total sales for each product?\\n* What is the average sales price for each product category?\\n* Who are the top 10 customers in terms of total sales?\\n* What is the sales trend over time?'}}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Danger</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will erase your chat history after a limit. Created this part because sometimes my Gemini LLM generates an error-500: Internal server error, it causes due to overloading of the model.\n",
    "# chats = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### /Users/mihiresh/Mihiresh/Hackathon/gdsc2/api_testing/rag2.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
